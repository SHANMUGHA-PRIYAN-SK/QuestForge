{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 1830,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01092896174863388,
      "grad_norm": 20.8580379486084,
      "learning_rate": 4.994535519125683e-05,
      "loss": 4.312,
      "step": 2
    },
    {
      "epoch": 0.02185792349726776,
      "grad_norm": 21.523305892944336,
      "learning_rate": 4.989071038251366e-05,
      "loss": 3.8013,
      "step": 4
    },
    {
      "epoch": 0.03278688524590164,
      "grad_norm": 19.11539077758789,
      "learning_rate": 4.9836065573770496e-05,
      "loss": 3.8804,
      "step": 6
    },
    {
      "epoch": 0.04371584699453552,
      "grad_norm": 18.826425552368164,
      "learning_rate": 4.978142076502732e-05,
      "loss": 4.0401,
      "step": 8
    },
    {
      "epoch": 0.0546448087431694,
      "grad_norm": 17.891225814819336,
      "learning_rate": 4.9726775956284156e-05,
      "loss": 3.0356,
      "step": 10
    },
    {
      "epoch": 0.06557377049180328,
      "grad_norm": 17.95521354675293,
      "learning_rate": 4.967213114754098e-05,
      "loss": 3.4933,
      "step": 12
    },
    {
      "epoch": 0.07650273224043716,
      "grad_norm": 18.19849395751953,
      "learning_rate": 4.9617486338797816e-05,
      "loss": 3.4938,
      "step": 14
    },
    {
      "epoch": 0.08743169398907104,
      "grad_norm": 18.359066009521484,
      "learning_rate": 4.956284153005465e-05,
      "loss": 3.3607,
      "step": 16
    },
    {
      "epoch": 0.09836065573770492,
      "grad_norm": 16.999698638916016,
      "learning_rate": 4.9508196721311476e-05,
      "loss": 3.0766,
      "step": 18
    },
    {
      "epoch": 0.1092896174863388,
      "grad_norm": 18.78799057006836,
      "learning_rate": 4.945355191256831e-05,
      "loss": 3.4186,
      "step": 20
    },
    {
      "epoch": 0.12021857923497267,
      "grad_norm": 16.290607452392578,
      "learning_rate": 4.9398907103825136e-05,
      "loss": 3.0596,
      "step": 22
    },
    {
      "epoch": 0.13114754098360656,
      "grad_norm": 17.99243927001953,
      "learning_rate": 4.934426229508197e-05,
      "loss": 3.1311,
      "step": 24
    },
    {
      "epoch": 0.14207650273224043,
      "grad_norm": 14.268632888793945,
      "learning_rate": 4.92896174863388e-05,
      "loss": 3.0793,
      "step": 26
    },
    {
      "epoch": 0.15300546448087432,
      "grad_norm": 17.280296325683594,
      "learning_rate": 4.923497267759563e-05,
      "loss": 3.3887,
      "step": 28
    },
    {
      "epoch": 0.16393442622950818,
      "grad_norm": 18.73628807067871,
      "learning_rate": 4.918032786885246e-05,
      "loss": 2.6663,
      "step": 30
    },
    {
      "epoch": 0.17486338797814208,
      "grad_norm": 13.711495399475098,
      "learning_rate": 4.912568306010929e-05,
      "loss": 2.5026,
      "step": 32
    },
    {
      "epoch": 0.18579234972677597,
      "grad_norm": 17.27953338623047,
      "learning_rate": 4.907103825136612e-05,
      "loss": 3.1984,
      "step": 34
    },
    {
      "epoch": 0.19672131147540983,
      "grad_norm": 16.35291862487793,
      "learning_rate": 4.9016393442622957e-05,
      "loss": 3.3856,
      "step": 36
    },
    {
      "epoch": 0.20765027322404372,
      "grad_norm": 14.974737167358398,
      "learning_rate": 4.896174863387978e-05,
      "loss": 2.9384,
      "step": 38
    },
    {
      "epoch": 0.2185792349726776,
      "grad_norm": 15.9683837890625,
      "learning_rate": 4.890710382513661e-05,
      "loss": 3.2116,
      "step": 40
    },
    {
      "epoch": 0.22950819672131148,
      "grad_norm": 15.26590347290039,
      "learning_rate": 4.885245901639344e-05,
      "loss": 3.0615,
      "step": 42
    },
    {
      "epoch": 0.24043715846994534,
      "grad_norm": 13.624571800231934,
      "learning_rate": 4.879781420765028e-05,
      "loss": 2.499,
      "step": 44
    },
    {
      "epoch": 0.25136612021857924,
      "grad_norm": 15.578988075256348,
      "learning_rate": 4.874316939890711e-05,
      "loss": 2.7831,
      "step": 46
    },
    {
      "epoch": 0.26229508196721313,
      "grad_norm": 16.422000885009766,
      "learning_rate": 4.868852459016394e-05,
      "loss": 3.1327,
      "step": 48
    },
    {
      "epoch": 0.273224043715847,
      "grad_norm": 15.09144401550293,
      "learning_rate": 4.863387978142076e-05,
      "loss": 2.2144,
      "step": 50
    },
    {
      "epoch": 0.28415300546448086,
      "grad_norm": 18.9578800201416,
      "learning_rate": 4.85792349726776e-05,
      "loss": 2.8703,
      "step": 52
    },
    {
      "epoch": 0.29508196721311475,
      "grad_norm": 17.575828552246094,
      "learning_rate": 4.852459016393443e-05,
      "loss": 3.2397,
      "step": 54
    },
    {
      "epoch": 0.30601092896174864,
      "grad_norm": 16.76225471496582,
      "learning_rate": 4.846994535519126e-05,
      "loss": 3.2064,
      "step": 56
    },
    {
      "epoch": 0.31693989071038253,
      "grad_norm": 17.185264587402344,
      "learning_rate": 4.841530054644809e-05,
      "loss": 3.1529,
      "step": 58
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 16.90216827392578,
      "learning_rate": 4.836065573770492e-05,
      "loss": 3.1708,
      "step": 60
    },
    {
      "epoch": 0.33879781420765026,
      "grad_norm": 13.42788028717041,
      "learning_rate": 4.830601092896175e-05,
      "loss": 2.5219,
      "step": 62
    },
    {
      "epoch": 0.34972677595628415,
      "grad_norm": 17.137285232543945,
      "learning_rate": 4.8251366120218584e-05,
      "loss": 3.1378,
      "step": 64
    },
    {
      "epoch": 0.36065573770491804,
      "grad_norm": 13.510666847229004,
      "learning_rate": 4.819672131147541e-05,
      "loss": 2.4289,
      "step": 66
    },
    {
      "epoch": 0.37158469945355194,
      "grad_norm": 16.600162506103516,
      "learning_rate": 4.8142076502732244e-05,
      "loss": 3.3552,
      "step": 68
    },
    {
      "epoch": 0.3825136612021858,
      "grad_norm": 13.612361907958984,
      "learning_rate": 4.808743169398907e-05,
      "loss": 2.5895,
      "step": 70
    },
    {
      "epoch": 0.39344262295081966,
      "grad_norm": 16.48382568359375,
      "learning_rate": 4.8032786885245904e-05,
      "loss": 3.0049,
      "step": 72
    },
    {
      "epoch": 0.40437158469945356,
      "grad_norm": 17.772499084472656,
      "learning_rate": 4.797814207650274e-05,
      "loss": 3.0899,
      "step": 74
    },
    {
      "epoch": 0.41530054644808745,
      "grad_norm": 15.525202751159668,
      "learning_rate": 4.7923497267759564e-05,
      "loss": 2.8507,
      "step": 76
    },
    {
      "epoch": 0.4262295081967213,
      "grad_norm": 14.859363555908203,
      "learning_rate": 4.78688524590164e-05,
      "loss": 2.6735,
      "step": 78
    },
    {
      "epoch": 0.4371584699453552,
      "grad_norm": 15.485483169555664,
      "learning_rate": 4.7814207650273224e-05,
      "loss": 3.1268,
      "step": 80
    },
    {
      "epoch": 0.44808743169398907,
      "grad_norm": 15.028870582580566,
      "learning_rate": 4.775956284153006e-05,
      "loss": 2.7984,
      "step": 82
    },
    {
      "epoch": 0.45901639344262296,
      "grad_norm": 16.480587005615234,
      "learning_rate": 4.770491803278689e-05,
      "loss": 3.2818,
      "step": 84
    },
    {
      "epoch": 0.46994535519125685,
      "grad_norm": 17.699275970458984,
      "learning_rate": 4.765027322404372e-05,
      "loss": 2.8272,
      "step": 86
    },
    {
      "epoch": 0.4808743169398907,
      "grad_norm": 17.09576416015625,
      "learning_rate": 4.7595628415300544e-05,
      "loss": 2.3744,
      "step": 88
    },
    {
      "epoch": 0.4918032786885246,
      "grad_norm": 14.70911979675293,
      "learning_rate": 4.754098360655738e-05,
      "loss": 3.0391,
      "step": 90
    },
    {
      "epoch": 0.5027322404371585,
      "grad_norm": 15.917710304260254,
      "learning_rate": 4.748633879781421e-05,
      "loss": 2.9274,
      "step": 92
    },
    {
      "epoch": 0.5136612021857924,
      "grad_norm": 15.24199104309082,
      "learning_rate": 4.7431693989071044e-05,
      "loss": 3.118,
      "step": 94
    },
    {
      "epoch": 0.5245901639344263,
      "grad_norm": 14.215072631835938,
      "learning_rate": 4.737704918032787e-05,
      "loss": 2.5482,
      "step": 96
    },
    {
      "epoch": 0.5355191256830601,
      "grad_norm": 16.250839233398438,
      "learning_rate": 4.73224043715847e-05,
      "loss": 2.8388,
      "step": 98
    },
    {
      "epoch": 0.546448087431694,
      "grad_norm": 14.357669830322266,
      "learning_rate": 4.726775956284154e-05,
      "loss": 2.7608,
      "step": 100
    },
    {
      "epoch": 0.5573770491803278,
      "grad_norm": 15.40329360961914,
      "learning_rate": 4.7213114754098365e-05,
      "loss": 2.9577,
      "step": 102
    },
    {
      "epoch": 0.5683060109289617,
      "grad_norm": 12.38412094116211,
      "learning_rate": 4.715846994535519e-05,
      "loss": 2.4221,
      "step": 104
    },
    {
      "epoch": 0.5792349726775956,
      "grad_norm": 14.10750961303711,
      "learning_rate": 4.7103825136612025e-05,
      "loss": 3.1606,
      "step": 106
    },
    {
      "epoch": 0.5901639344262295,
      "grad_norm": 14.26600170135498,
      "learning_rate": 4.704918032786885e-05,
      "loss": 2.515,
      "step": 108
    },
    {
      "epoch": 0.6010928961748634,
      "grad_norm": 13.946163177490234,
      "learning_rate": 4.6994535519125685e-05,
      "loss": 2.6523,
      "step": 110
    },
    {
      "epoch": 0.6120218579234973,
      "grad_norm": 16.104761123657227,
      "learning_rate": 4.693989071038252e-05,
      "loss": 3.0249,
      "step": 112
    },
    {
      "epoch": 0.6229508196721312,
      "grad_norm": 13.300262451171875,
      "learning_rate": 4.6885245901639345e-05,
      "loss": 2.7127,
      "step": 114
    },
    {
      "epoch": 0.6338797814207651,
      "grad_norm": 16.54242515563965,
      "learning_rate": 4.683060109289618e-05,
      "loss": 2.9862,
      "step": 116
    },
    {
      "epoch": 0.644808743169399,
      "grad_norm": 14.502885818481445,
      "learning_rate": 4.6775956284153005e-05,
      "loss": 3.1221,
      "step": 118
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 11.15943431854248,
      "learning_rate": 4.672131147540984e-05,
      "loss": 2.7779,
      "step": 120
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 12.701925277709961,
      "learning_rate": 4.666666666666667e-05,
      "loss": 2.6622,
      "step": 122
    },
    {
      "epoch": 0.6775956284153005,
      "grad_norm": 14.822992324829102,
      "learning_rate": 4.66120218579235e-05,
      "loss": 2.8159,
      "step": 124
    },
    {
      "epoch": 0.6885245901639344,
      "grad_norm": 16.483205795288086,
      "learning_rate": 4.655737704918033e-05,
      "loss": 2.9962,
      "step": 126
    },
    {
      "epoch": 0.6994535519125683,
      "grad_norm": 17.0061092376709,
      "learning_rate": 4.650273224043716e-05,
      "loss": 2.992,
      "step": 128
    },
    {
      "epoch": 0.7103825136612022,
      "grad_norm": 15.50518798828125,
      "learning_rate": 4.644808743169399e-05,
      "loss": 2.688,
      "step": 130
    },
    {
      "epoch": 0.7213114754098361,
      "grad_norm": 15.105276107788086,
      "learning_rate": 4.6393442622950825e-05,
      "loss": 3.1711,
      "step": 132
    },
    {
      "epoch": 0.73224043715847,
      "grad_norm": 15.28918743133545,
      "learning_rate": 4.633879781420765e-05,
      "loss": 3.0245,
      "step": 134
    },
    {
      "epoch": 0.7431693989071039,
      "grad_norm": 15.11325454711914,
      "learning_rate": 4.628415300546448e-05,
      "loss": 3.1905,
      "step": 136
    },
    {
      "epoch": 0.7540983606557377,
      "grad_norm": 13.333478927612305,
      "learning_rate": 4.622950819672132e-05,
      "loss": 3.0706,
      "step": 138
    },
    {
      "epoch": 0.7650273224043715,
      "grad_norm": 13.041096687316895,
      "learning_rate": 4.6174863387978145e-05,
      "loss": 2.6258,
      "step": 140
    },
    {
      "epoch": 0.7759562841530054,
      "grad_norm": 16.391462326049805,
      "learning_rate": 4.612021857923497e-05,
      "loss": 2.4268,
      "step": 142
    },
    {
      "epoch": 0.7868852459016393,
      "grad_norm": 15.407227516174316,
      "learning_rate": 4.6065573770491805e-05,
      "loss": 2.7796,
      "step": 144
    },
    {
      "epoch": 0.7978142076502732,
      "grad_norm": 16.17913818359375,
      "learning_rate": 4.601092896174863e-05,
      "loss": 3.1872,
      "step": 146
    },
    {
      "epoch": 0.8087431693989071,
      "grad_norm": 12.437795639038086,
      "learning_rate": 4.595628415300547e-05,
      "loss": 2.4851,
      "step": 148
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 14.943272590637207,
      "learning_rate": 4.59016393442623e-05,
      "loss": 3.0639,
      "step": 150
    },
    {
      "epoch": 0.8306010928961749,
      "grad_norm": 15.811052322387695,
      "learning_rate": 4.5846994535519125e-05,
      "loss": 2.4368,
      "step": 152
    },
    {
      "epoch": 0.8415300546448088,
      "grad_norm": 15.781413078308105,
      "learning_rate": 4.579234972677596e-05,
      "loss": 3.053,
      "step": 154
    },
    {
      "epoch": 0.8524590163934426,
      "grad_norm": 12.734055519104004,
      "learning_rate": 4.5737704918032786e-05,
      "loss": 3.1665,
      "step": 156
    },
    {
      "epoch": 0.8633879781420765,
      "grad_norm": 14.073836326599121,
      "learning_rate": 4.568306010928962e-05,
      "loss": 3.0117,
      "step": 158
    },
    {
      "epoch": 0.8743169398907104,
      "grad_norm": 16.37983512878418,
      "learning_rate": 4.562841530054645e-05,
      "loss": 2.5884,
      "step": 160
    },
    {
      "epoch": 0.8852459016393442,
      "grad_norm": 15.643173217773438,
      "learning_rate": 4.557377049180328e-05,
      "loss": 3.1523,
      "step": 162
    },
    {
      "epoch": 0.8961748633879781,
      "grad_norm": 15.916587829589844,
      "learning_rate": 4.551912568306011e-05,
      "loss": 2.9059,
      "step": 164
    },
    {
      "epoch": 0.907103825136612,
      "grad_norm": 11.643310546875,
      "learning_rate": 4.5464480874316946e-05,
      "loss": 2.4408,
      "step": 166
    },
    {
      "epoch": 0.9180327868852459,
      "grad_norm": 13.435857772827148,
      "learning_rate": 4.540983606557377e-05,
      "loss": 3.0301,
      "step": 168
    },
    {
      "epoch": 0.9289617486338798,
      "grad_norm": 12.914360046386719,
      "learning_rate": 4.5355191256830606e-05,
      "loss": 2.6433,
      "step": 170
    },
    {
      "epoch": 0.9398907103825137,
      "grad_norm": 12.198830604553223,
      "learning_rate": 4.530054644808743e-05,
      "loss": 2.6987,
      "step": 172
    },
    {
      "epoch": 0.9508196721311475,
      "grad_norm": 12.032828330993652,
      "learning_rate": 4.524590163934426e-05,
      "loss": 2.5047,
      "step": 174
    },
    {
      "epoch": 0.9617486338797814,
      "grad_norm": 16.197093963623047,
      "learning_rate": 4.51912568306011e-05,
      "loss": 3.1892,
      "step": 176
    },
    {
      "epoch": 0.9726775956284153,
      "grad_norm": 14.889744758605957,
      "learning_rate": 4.5136612021857926e-05,
      "loss": 2.679,
      "step": 178
    },
    {
      "epoch": 0.9836065573770492,
      "grad_norm": 14.666614532470703,
      "learning_rate": 4.508196721311476e-05,
      "loss": 2.6921,
      "step": 180
    },
    {
      "epoch": 0.994535519125683,
      "grad_norm": 14.420398712158203,
      "learning_rate": 4.5027322404371586e-05,
      "loss": 3.0833,
      "step": 182
    },
    {
      "epoch": 1.005464480874317,
      "grad_norm": 14.339756965637207,
      "learning_rate": 4.497267759562841e-05,
      "loss": 2.6263,
      "step": 184
    },
    {
      "epoch": 1.0163934426229508,
      "grad_norm": 14.04675579071045,
      "learning_rate": 4.491803278688525e-05,
      "loss": 2.1085,
      "step": 186
    },
    {
      "epoch": 1.0273224043715847,
      "grad_norm": 14.314974784851074,
      "learning_rate": 4.486338797814208e-05,
      "loss": 2.7176,
      "step": 188
    },
    {
      "epoch": 1.0382513661202186,
      "grad_norm": 14.37350845336914,
      "learning_rate": 4.4808743169398906e-05,
      "loss": 2.5737,
      "step": 190
    },
    {
      "epoch": 1.0491803278688525,
      "grad_norm": 14.561626434326172,
      "learning_rate": 4.475409836065574e-05,
      "loss": 2.4068,
      "step": 192
    },
    {
      "epoch": 1.0601092896174864,
      "grad_norm": 15.809443473815918,
      "learning_rate": 4.4699453551912566e-05,
      "loss": 2.7456,
      "step": 194
    },
    {
      "epoch": 1.0710382513661203,
      "grad_norm": 16.608205795288086,
      "learning_rate": 4.46448087431694e-05,
      "loss": 2.4814,
      "step": 196
    },
    {
      "epoch": 1.0819672131147542,
      "grad_norm": 14.266555786132812,
      "learning_rate": 4.459016393442623e-05,
      "loss": 2.7684,
      "step": 198
    },
    {
      "epoch": 1.092896174863388,
      "grad_norm": 13.130417823791504,
      "learning_rate": 4.453551912568306e-05,
      "loss": 1.9361,
      "step": 200
    },
    {
      "epoch": 1.1038251366120218,
      "grad_norm": 14.577361106872559,
      "learning_rate": 4.448087431693989e-05,
      "loss": 2.2055,
      "step": 202
    },
    {
      "epoch": 1.1147540983606556,
      "grad_norm": 15.697590827941895,
      "learning_rate": 4.442622950819673e-05,
      "loss": 2.5084,
      "step": 204
    },
    {
      "epoch": 1.1256830601092895,
      "grad_norm": 13.155709266662598,
      "learning_rate": 4.437158469945355e-05,
      "loss": 1.9933,
      "step": 206
    },
    {
      "epoch": 1.1366120218579234,
      "grad_norm": 16.358827590942383,
      "learning_rate": 4.431693989071039e-05,
      "loss": 2.2987,
      "step": 208
    },
    {
      "epoch": 1.1475409836065573,
      "grad_norm": 16.19785499572754,
      "learning_rate": 4.426229508196721e-05,
      "loss": 2.33,
      "step": 210
    },
    {
      "epoch": 1.1584699453551912,
      "grad_norm": 15.149491310119629,
      "learning_rate": 4.420765027322405e-05,
      "loss": 2.3025,
      "step": 212
    },
    {
      "epoch": 1.169398907103825,
      "grad_norm": 16.966737747192383,
      "learning_rate": 4.415300546448088e-05,
      "loss": 2.4608,
      "step": 214
    },
    {
      "epoch": 1.180327868852459,
      "grad_norm": 14.190747261047363,
      "learning_rate": 4.409836065573771e-05,
      "loss": 2.1484,
      "step": 216
    },
    {
      "epoch": 1.1912568306010929,
      "grad_norm": 13.561020851135254,
      "learning_rate": 4.404371584699454e-05,
      "loss": 2.0213,
      "step": 218
    },
    {
      "epoch": 1.2021857923497268,
      "grad_norm": 15.379605293273926,
      "learning_rate": 4.398907103825137e-05,
      "loss": 2.3785,
      "step": 220
    },
    {
      "epoch": 1.2131147540983607,
      "grad_norm": 15.191174507141113,
      "learning_rate": 4.3934426229508194e-05,
      "loss": 2.4268,
      "step": 222
    },
    {
      "epoch": 1.2240437158469946,
      "grad_norm": 16.545543670654297,
      "learning_rate": 4.3879781420765034e-05,
      "loss": 2.6037,
      "step": 224
    },
    {
      "epoch": 1.2349726775956285,
      "grad_norm": 13.603958129882812,
      "learning_rate": 4.382513661202186e-05,
      "loss": 2.2509,
      "step": 226
    },
    {
      "epoch": 1.2459016393442623,
      "grad_norm": 14.311060905456543,
      "learning_rate": 4.377049180327869e-05,
      "loss": 2.2849,
      "step": 228
    },
    {
      "epoch": 1.2568306010928962,
      "grad_norm": 17.185958862304688,
      "learning_rate": 4.371584699453552e-05,
      "loss": 2.3726,
      "step": 230
    },
    {
      "epoch": 1.2677595628415301,
      "grad_norm": 13.98758316040039,
      "learning_rate": 4.366120218579235e-05,
      "loss": 2.4055,
      "step": 232
    },
    {
      "epoch": 1.278688524590164,
      "grad_norm": 15.091439247131348,
      "learning_rate": 4.360655737704919e-05,
      "loss": 2.1764,
      "step": 234
    },
    {
      "epoch": 1.289617486338798,
      "grad_norm": 13.876226425170898,
      "learning_rate": 4.3551912568306014e-05,
      "loss": 2.0528,
      "step": 236
    },
    {
      "epoch": 1.3005464480874318,
      "grad_norm": 16.065095901489258,
      "learning_rate": 4.349726775956284e-05,
      "loss": 2.3956,
      "step": 238
    },
    {
      "epoch": 1.3114754098360657,
      "grad_norm": 15.879761695861816,
      "learning_rate": 4.3442622950819674e-05,
      "loss": 2.4436,
      "step": 240
    },
    {
      "epoch": 1.3224043715846996,
      "grad_norm": 16.051259994506836,
      "learning_rate": 4.338797814207651e-05,
      "loss": 2.0121,
      "step": 242
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 16.338829040527344,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 2.087,
      "step": 244
    },
    {
      "epoch": 1.3442622950819672,
      "grad_norm": 17.088359832763672,
      "learning_rate": 4.327868852459017e-05,
      "loss": 2.5776,
      "step": 246
    },
    {
      "epoch": 1.355191256830601,
      "grad_norm": 13.517765045166016,
      "learning_rate": 4.3224043715846994e-05,
      "loss": 2.3674,
      "step": 248
    },
    {
      "epoch": 1.366120218579235,
      "grad_norm": 16.829158782958984,
      "learning_rate": 4.316939890710383e-05,
      "loss": 2.2349,
      "step": 250
    },
    {
      "epoch": 1.3770491803278688,
      "grad_norm": 16.131601333618164,
      "learning_rate": 4.311475409836066e-05,
      "loss": 2.318,
      "step": 252
    },
    {
      "epoch": 1.3879781420765027,
      "grad_norm": 13.542826652526855,
      "learning_rate": 4.306010928961749e-05,
      "loss": 2.0978,
      "step": 254
    },
    {
      "epoch": 1.3989071038251366,
      "grad_norm": 15.041827201843262,
      "learning_rate": 4.300546448087432e-05,
      "loss": 2.3063,
      "step": 256
    },
    {
      "epoch": 1.4098360655737705,
      "grad_norm": 15.91789722442627,
      "learning_rate": 4.295081967213115e-05,
      "loss": 2.3608,
      "step": 258
    },
    {
      "epoch": 1.4207650273224044,
      "grad_norm": 13.574538230895996,
      "learning_rate": 4.289617486338798e-05,
      "loss": 2.2035,
      "step": 260
    },
    {
      "epoch": 1.4316939890710383,
      "grad_norm": 15.50790023803711,
      "learning_rate": 4.2841530054644815e-05,
      "loss": 2.0051,
      "step": 262
    },
    {
      "epoch": 1.4426229508196722,
      "grad_norm": 14.597260475158691,
      "learning_rate": 4.278688524590164e-05,
      "loss": 2.1121,
      "step": 264
    },
    {
      "epoch": 1.453551912568306,
      "grad_norm": 15.717132568359375,
      "learning_rate": 4.2732240437158475e-05,
      "loss": 2.432,
      "step": 266
    },
    {
      "epoch": 1.46448087431694,
      "grad_norm": 15.114916801452637,
      "learning_rate": 4.26775956284153e-05,
      "loss": 2.2364,
      "step": 268
    },
    {
      "epoch": 1.4754098360655736,
      "grad_norm": 12.860837936401367,
      "learning_rate": 4.262295081967213e-05,
      "loss": 2.1027,
      "step": 270
    },
    {
      "epoch": 1.4863387978142075,
      "grad_norm": 16.13810920715332,
      "learning_rate": 4.256830601092897e-05,
      "loss": 2.4504,
      "step": 272
    },
    {
      "epoch": 1.4972677595628414,
      "grad_norm": 13.963516235351562,
      "learning_rate": 4.2513661202185795e-05,
      "loss": 2.2274,
      "step": 274
    },
    {
      "epoch": 1.5081967213114753,
      "grad_norm": 16.102691650390625,
      "learning_rate": 4.245901639344262e-05,
      "loss": 2.6626,
      "step": 276
    },
    {
      "epoch": 1.5191256830601092,
      "grad_norm": 15.709909439086914,
      "learning_rate": 4.2404371584699455e-05,
      "loss": 2.4827,
      "step": 278
    },
    {
      "epoch": 1.530054644808743,
      "grad_norm": 17.023317337036133,
      "learning_rate": 4.234972677595629e-05,
      "loss": 2.434,
      "step": 280
    },
    {
      "epoch": 1.540983606557377,
      "grad_norm": 14.324649810791016,
      "learning_rate": 4.229508196721312e-05,
      "loss": 1.9754,
      "step": 282
    },
    {
      "epoch": 1.5519125683060109,
      "grad_norm": 13.066046714782715,
      "learning_rate": 4.224043715846995e-05,
      "loss": 1.9689,
      "step": 284
    },
    {
      "epoch": 1.5628415300546448,
      "grad_norm": 14.578374862670898,
      "learning_rate": 4.2185792349726775e-05,
      "loss": 2.5347,
      "step": 286
    },
    {
      "epoch": 1.5737704918032787,
      "grad_norm": 16.104671478271484,
      "learning_rate": 4.213114754098361e-05,
      "loss": 2.3258,
      "step": 288
    },
    {
      "epoch": 1.5846994535519126,
      "grad_norm": 16.378759384155273,
      "learning_rate": 4.207650273224044e-05,
      "loss": 2.5303,
      "step": 290
    },
    {
      "epoch": 1.5956284153005464,
      "grad_norm": 15.383304595947266,
      "learning_rate": 4.202185792349727e-05,
      "loss": 2.1084,
      "step": 292
    },
    {
      "epoch": 1.6065573770491803,
      "grad_norm": 13.638025283813477,
      "learning_rate": 4.19672131147541e-05,
      "loss": 2.3098,
      "step": 294
    },
    {
      "epoch": 1.6174863387978142,
      "grad_norm": 14.844443321228027,
      "learning_rate": 4.191256830601093e-05,
      "loss": 2.4468,
      "step": 296
    },
    {
      "epoch": 1.6284153005464481,
      "grad_norm": 15.961009979248047,
      "learning_rate": 4.185792349726776e-05,
      "loss": 2.4978,
      "step": 298
    },
    {
      "epoch": 1.639344262295082,
      "grad_norm": 16.855377197265625,
      "learning_rate": 4.1803278688524595e-05,
      "loss": 2.2055,
      "step": 300
    },
    {
      "epoch": 1.650273224043716,
      "grad_norm": 16.529294967651367,
      "learning_rate": 4.174863387978142e-05,
      "loss": 2.407,
      "step": 302
    },
    {
      "epoch": 1.6612021857923498,
      "grad_norm": 13.331280708312988,
      "learning_rate": 4.1693989071038255e-05,
      "loss": 1.7081,
      "step": 304
    },
    {
      "epoch": 1.6721311475409837,
      "grad_norm": 17.442941665649414,
      "learning_rate": 4.163934426229508e-05,
      "loss": 2.4365,
      "step": 306
    },
    {
      "epoch": 1.6830601092896176,
      "grad_norm": 13.169398307800293,
      "learning_rate": 4.158469945355191e-05,
      "loss": 2.2571,
      "step": 308
    },
    {
      "epoch": 1.6939890710382515,
      "grad_norm": 13.583589553833008,
      "learning_rate": 4.153005464480875e-05,
      "loss": 1.8392,
      "step": 310
    },
    {
      "epoch": 1.7049180327868854,
      "grad_norm": 15.91956901550293,
      "learning_rate": 4.1475409836065575e-05,
      "loss": 2.5092,
      "step": 312
    },
    {
      "epoch": 1.7158469945355193,
      "grad_norm": 16.623798370361328,
      "learning_rate": 4.142076502732241e-05,
      "loss": 2.2008,
      "step": 314
    },
    {
      "epoch": 1.7267759562841531,
      "grad_norm": 16.185579299926758,
      "learning_rate": 4.1366120218579236e-05,
      "loss": 2.6522,
      "step": 316
    },
    {
      "epoch": 1.737704918032787,
      "grad_norm": 14.772375106811523,
      "learning_rate": 4.131147540983607e-05,
      "loss": 2.2308,
      "step": 318
    },
    {
      "epoch": 1.748633879781421,
      "grad_norm": 13.24148941040039,
      "learning_rate": 4.12568306010929e-05,
      "loss": 1.9342,
      "step": 320
    },
    {
      "epoch": 1.7595628415300546,
      "grad_norm": 13.842702865600586,
      "learning_rate": 4.120218579234973e-05,
      "loss": 2.1739,
      "step": 322
    },
    {
      "epoch": 1.7704918032786885,
      "grad_norm": 14.387089729309082,
      "learning_rate": 4.1147540983606556e-05,
      "loss": 2.2097,
      "step": 324
    },
    {
      "epoch": 1.7814207650273224,
      "grad_norm": 14.16783332824707,
      "learning_rate": 4.109289617486339e-05,
      "loss": 2.2337,
      "step": 326
    },
    {
      "epoch": 1.7923497267759563,
      "grad_norm": 16.619037628173828,
      "learning_rate": 4.103825136612022e-05,
      "loss": 2.4597,
      "step": 328
    },
    {
      "epoch": 1.8032786885245902,
      "grad_norm": 17.74479103088379,
      "learning_rate": 4.098360655737705e-05,
      "loss": 2.5639,
      "step": 330
    },
    {
      "epoch": 1.814207650273224,
      "grad_norm": 17.922452926635742,
      "learning_rate": 4.092896174863388e-05,
      "loss": 2.4733,
      "step": 332
    },
    {
      "epoch": 1.825136612021858,
      "grad_norm": 17.130361557006836,
      "learning_rate": 4.087431693989071e-05,
      "loss": 2.2405,
      "step": 334
    },
    {
      "epoch": 1.8360655737704918,
      "grad_norm": 15.88245964050293,
      "learning_rate": 4.081967213114754e-05,
      "loss": 2.2523,
      "step": 336
    },
    {
      "epoch": 1.8469945355191257,
      "grad_norm": 15.546086311340332,
      "learning_rate": 4.0765027322404376e-05,
      "loss": 2.5636,
      "step": 338
    },
    {
      "epoch": 1.8579234972677594,
      "grad_norm": 15.26401138305664,
      "learning_rate": 4.07103825136612e-05,
      "loss": 2.4794,
      "step": 340
    },
    {
      "epoch": 1.8688524590163933,
      "grad_norm": 15.991711616516113,
      "learning_rate": 4.0655737704918036e-05,
      "loss": 2.4663,
      "step": 342
    },
    {
      "epoch": 1.8797814207650272,
      "grad_norm": 16.227800369262695,
      "learning_rate": 4.060109289617486e-05,
      "loss": 2.1371,
      "step": 344
    },
    {
      "epoch": 1.890710382513661,
      "grad_norm": 16.6120662689209,
      "learning_rate": 4.0546448087431696e-05,
      "loss": 2.6608,
      "step": 346
    },
    {
      "epoch": 1.901639344262295,
      "grad_norm": 15.351263046264648,
      "learning_rate": 4.049180327868853e-05,
      "loss": 2.5205,
      "step": 348
    },
    {
      "epoch": 1.9125683060109289,
      "grad_norm": 16.1954402923584,
      "learning_rate": 4.0437158469945356e-05,
      "loss": 2.3052,
      "step": 350
    },
    {
      "epoch": 1.9234972677595628,
      "grad_norm": 13.187169075012207,
      "learning_rate": 4.038251366120219e-05,
      "loss": 1.9165,
      "step": 352
    },
    {
      "epoch": 1.9344262295081966,
      "grad_norm": 15.9389009475708,
      "learning_rate": 4.0327868852459016e-05,
      "loss": 2.2527,
      "step": 354
    },
    {
      "epoch": 1.9453551912568305,
      "grad_norm": 13.113243103027344,
      "learning_rate": 4.027322404371585e-05,
      "loss": 2.112,
      "step": 356
    },
    {
      "epoch": 1.9562841530054644,
      "grad_norm": 13.445035934448242,
      "learning_rate": 4.021857923497268e-05,
      "loss": 2.1786,
      "step": 358
    },
    {
      "epoch": 1.9672131147540983,
      "grad_norm": 15.143430709838867,
      "learning_rate": 4.016393442622951e-05,
      "loss": 2.4695,
      "step": 360
    },
    {
      "epoch": 1.9781420765027322,
      "grad_norm": 14.476194381713867,
      "learning_rate": 4.0109289617486336e-05,
      "loss": 2.1605,
      "step": 362
    },
    {
      "epoch": 1.989071038251366,
      "grad_norm": 14.082832336425781,
      "learning_rate": 4.005464480874317e-05,
      "loss": 2.0029,
      "step": 364
    },
    {
      "epoch": 2.0,
      "grad_norm": 16.515382766723633,
      "learning_rate": 4e-05,
      "loss": 2.6441,
      "step": 366
    },
    {
      "epoch": 2.010928961748634,
      "grad_norm": 13.170747756958008,
      "learning_rate": 3.994535519125684e-05,
      "loss": 1.7726,
      "step": 368
    },
    {
      "epoch": 2.021857923497268,
      "grad_norm": 13.062111854553223,
      "learning_rate": 3.989071038251366e-05,
      "loss": 1.7169,
      "step": 370
    },
    {
      "epoch": 2.0327868852459017,
      "grad_norm": 15.323177337646484,
      "learning_rate": 3.983606557377049e-05,
      "loss": 2.0752,
      "step": 372
    },
    {
      "epoch": 2.0437158469945356,
      "grad_norm": 15.606341361999512,
      "learning_rate": 3.9781420765027323e-05,
      "loss": 2.2755,
      "step": 374
    },
    {
      "epoch": 2.0546448087431695,
      "grad_norm": 16.75136375427246,
      "learning_rate": 3.972677595628416e-05,
      "loss": 2.0815,
      "step": 376
    },
    {
      "epoch": 2.0655737704918034,
      "grad_norm": 15.01508903503418,
      "learning_rate": 3.9672131147540983e-05,
      "loss": 1.7129,
      "step": 378
    },
    {
      "epoch": 2.0765027322404372,
      "grad_norm": 14.330965995788574,
      "learning_rate": 3.961748633879782e-05,
      "loss": 1.8733,
      "step": 380
    },
    {
      "epoch": 2.087431693989071,
      "grad_norm": 15.414239883422852,
      "learning_rate": 3.9562841530054644e-05,
      "loss": 1.8008,
      "step": 382
    },
    {
      "epoch": 2.098360655737705,
      "grad_norm": 16.215173721313477,
      "learning_rate": 3.950819672131148e-05,
      "loss": 1.9622,
      "step": 384
    },
    {
      "epoch": 2.109289617486339,
      "grad_norm": 15.543009757995605,
      "learning_rate": 3.945355191256831e-05,
      "loss": 2.3857,
      "step": 386
    },
    {
      "epoch": 2.120218579234973,
      "grad_norm": 15.126325607299805,
      "learning_rate": 3.939890710382514e-05,
      "loss": 2.1781,
      "step": 388
    },
    {
      "epoch": 2.1311475409836067,
      "grad_norm": 12.614011764526367,
      "learning_rate": 3.934426229508197e-05,
      "loss": 1.6184,
      "step": 390
    },
    {
      "epoch": 2.1420765027322406,
      "grad_norm": 14.163008689880371,
      "learning_rate": 3.92896174863388e-05,
      "loss": 1.8852,
      "step": 392
    },
    {
      "epoch": 2.1530054644808745,
      "grad_norm": 16.425880432128906,
      "learning_rate": 3.923497267759563e-05,
      "loss": 2.2007,
      "step": 394
    },
    {
      "epoch": 2.1639344262295084,
      "grad_norm": 15.670730590820312,
      "learning_rate": 3.9180327868852464e-05,
      "loss": 2.1188,
      "step": 396
    },
    {
      "epoch": 2.1748633879781423,
      "grad_norm": 16.306161880493164,
      "learning_rate": 3.912568306010929e-05,
      "loss": 2.0061,
      "step": 398
    },
    {
      "epoch": 2.185792349726776,
      "grad_norm": 16.416793823242188,
      "learning_rate": 3.9071038251366124e-05,
      "loss": 2.0683,
      "step": 400
    },
    {
      "epoch": 2.19672131147541,
      "grad_norm": 16.115161895751953,
      "learning_rate": 3.901639344262295e-05,
      "loss": 2.1065,
      "step": 402
    },
    {
      "epoch": 2.2076502732240435,
      "grad_norm": 16.95631980895996,
      "learning_rate": 3.8961748633879784e-05,
      "loss": 1.8449,
      "step": 404
    },
    {
      "epoch": 2.2185792349726774,
      "grad_norm": 15.319990158081055,
      "learning_rate": 3.890710382513662e-05,
      "loss": 2.0586,
      "step": 406
    },
    {
      "epoch": 2.2295081967213113,
      "grad_norm": 16.281431198120117,
      "learning_rate": 3.8852459016393444e-05,
      "loss": 2.2866,
      "step": 408
    },
    {
      "epoch": 2.240437158469945,
      "grad_norm": 16.178001403808594,
      "learning_rate": 3.879781420765027e-05,
      "loss": 2.1,
      "step": 410
    },
    {
      "epoch": 2.251366120218579,
      "grad_norm": 13.069771766662598,
      "learning_rate": 3.8743169398907104e-05,
      "loss": 1.8677,
      "step": 412
    },
    {
      "epoch": 2.262295081967213,
      "grad_norm": 16.64145851135254,
      "learning_rate": 3.868852459016394e-05,
      "loss": 2.0129,
      "step": 414
    },
    {
      "epoch": 2.273224043715847,
      "grad_norm": 15.725259780883789,
      "learning_rate": 3.8633879781420764e-05,
      "loss": 1.9422,
      "step": 416
    },
    {
      "epoch": 2.2841530054644807,
      "grad_norm": 15.204270362854004,
      "learning_rate": 3.85792349726776e-05,
      "loss": 2.1016,
      "step": 418
    },
    {
      "epoch": 2.2950819672131146,
      "grad_norm": 15.085640907287598,
      "learning_rate": 3.8524590163934424e-05,
      "loss": 2.2049,
      "step": 420
    },
    {
      "epoch": 2.3060109289617485,
      "grad_norm": 13.739588737487793,
      "learning_rate": 3.8469945355191264e-05,
      "loss": 1.9132,
      "step": 422
    },
    {
      "epoch": 2.3169398907103824,
      "grad_norm": 15.456378936767578,
      "learning_rate": 3.841530054644809e-05,
      "loss": 1.8169,
      "step": 424
    },
    {
      "epoch": 2.3278688524590163,
      "grad_norm": 16.401432037353516,
      "learning_rate": 3.836065573770492e-05,
      "loss": 1.6215,
      "step": 426
    },
    {
      "epoch": 2.33879781420765,
      "grad_norm": 16.025835037231445,
      "learning_rate": 3.830601092896175e-05,
      "loss": 1.6396,
      "step": 428
    },
    {
      "epoch": 2.349726775956284,
      "grad_norm": 13.110255241394043,
      "learning_rate": 3.825136612021858e-05,
      "loss": 1.4036,
      "step": 430
    },
    {
      "epoch": 2.360655737704918,
      "grad_norm": 15.805607795715332,
      "learning_rate": 3.819672131147541e-05,
      "loss": 1.6903,
      "step": 432
    },
    {
      "epoch": 2.371584699453552,
      "grad_norm": 15.735227584838867,
      "learning_rate": 3.8142076502732245e-05,
      "loss": 2.0377,
      "step": 434
    },
    {
      "epoch": 2.3825136612021858,
      "grad_norm": 13.949470520019531,
      "learning_rate": 3.808743169398907e-05,
      "loss": 2.0044,
      "step": 436
    },
    {
      "epoch": 2.3934426229508197,
      "grad_norm": 17.897916793823242,
      "learning_rate": 3.8032786885245905e-05,
      "loss": 2.3672,
      "step": 438
    },
    {
      "epoch": 2.4043715846994536,
      "grad_norm": 14.07094669342041,
      "learning_rate": 3.797814207650273e-05,
      "loss": 1.9248,
      "step": 440
    },
    {
      "epoch": 2.4153005464480874,
      "grad_norm": 17.88750457763672,
      "learning_rate": 3.7923497267759565e-05,
      "loss": 2.181,
      "step": 442
    },
    {
      "epoch": 2.4262295081967213,
      "grad_norm": 16.590587615966797,
      "learning_rate": 3.78688524590164e-05,
      "loss": 2.1903,
      "step": 444
    },
    {
      "epoch": 2.4371584699453552,
      "grad_norm": 13.204817771911621,
      "learning_rate": 3.7814207650273225e-05,
      "loss": 1.9401,
      "step": 446
    },
    {
      "epoch": 2.448087431693989,
      "grad_norm": 16.987272262573242,
      "learning_rate": 3.775956284153006e-05,
      "loss": 2.2004,
      "step": 448
    },
    {
      "epoch": 2.459016393442623,
      "grad_norm": 15.55906867980957,
      "learning_rate": 3.7704918032786885e-05,
      "loss": 1.777,
      "step": 450
    },
    {
      "epoch": 2.469945355191257,
      "grad_norm": 15.601852416992188,
      "learning_rate": 3.765027322404372e-05,
      "loss": 2.036,
      "step": 452
    },
    {
      "epoch": 2.480874316939891,
      "grad_norm": 16.36517906188965,
      "learning_rate": 3.759562841530055e-05,
      "loss": 1.9245,
      "step": 454
    },
    {
      "epoch": 2.4918032786885247,
      "grad_norm": 17.6691951751709,
      "learning_rate": 3.754098360655738e-05,
      "loss": 2.3122,
      "step": 456
    },
    {
      "epoch": 2.5027322404371586,
      "grad_norm": 15.90240478515625,
      "learning_rate": 3.7486338797814205e-05,
      "loss": 2.0753,
      "step": 458
    },
    {
      "epoch": 2.5136612021857925,
      "grad_norm": 15.766058921813965,
      "learning_rate": 3.7431693989071045e-05,
      "loss": 1.9182,
      "step": 460
    },
    {
      "epoch": 2.5245901639344264,
      "grad_norm": 15.582995414733887,
      "learning_rate": 3.737704918032787e-05,
      "loss": 2.2089,
      "step": 462
    },
    {
      "epoch": 2.5355191256830603,
      "grad_norm": 16.986629486083984,
      "learning_rate": 3.73224043715847e-05,
      "loss": 2.0238,
      "step": 464
    },
    {
      "epoch": 2.546448087431694,
      "grad_norm": 16.071090698242188,
      "learning_rate": 3.726775956284153e-05,
      "loss": 2.0044,
      "step": 466
    },
    {
      "epoch": 2.557377049180328,
      "grad_norm": 13.541960716247559,
      "learning_rate": 3.721311475409836e-05,
      "loss": 1.7097,
      "step": 468
    },
    {
      "epoch": 2.5683060109289615,
      "grad_norm": 16.57415199279785,
      "learning_rate": 3.71584699453552e-05,
      "loss": 1.805,
      "step": 470
    },
    {
      "epoch": 2.579234972677596,
      "grad_norm": 15.582040786743164,
      "learning_rate": 3.7103825136612025e-05,
      "loss": 1.9909,
      "step": 472
    },
    {
      "epoch": 2.5901639344262293,
      "grad_norm": 17.36253547668457,
      "learning_rate": 3.704918032786885e-05,
      "loss": 2.1774,
      "step": 474
    },
    {
      "epoch": 2.6010928961748636,
      "grad_norm": 12.700546264648438,
      "learning_rate": 3.6994535519125686e-05,
      "loss": 1.7547,
      "step": 476
    },
    {
      "epoch": 2.612021857923497,
      "grad_norm": 13.134143829345703,
      "learning_rate": 3.693989071038251e-05,
      "loss": 1.942,
      "step": 478
    },
    {
      "epoch": 2.6229508196721314,
      "grad_norm": 14.740527153015137,
      "learning_rate": 3.6885245901639346e-05,
      "loss": 1.8753,
      "step": 480
    },
    {
      "epoch": 2.633879781420765,
      "grad_norm": 15.602957725524902,
      "learning_rate": 3.683060109289618e-05,
      "loss": 2.3291,
      "step": 482
    },
    {
      "epoch": 2.644808743169399,
      "grad_norm": 14.008695602416992,
      "learning_rate": 3.6775956284153006e-05,
      "loss": 1.8359,
      "step": 484
    },
    {
      "epoch": 2.6557377049180326,
      "grad_norm": 16.5043888092041,
      "learning_rate": 3.672131147540984e-05,
      "loss": 2.1871,
      "step": 486
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 15.88241958618164,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 1.711,
      "step": 488
    },
    {
      "epoch": 2.6775956284153004,
      "grad_norm": 16.663818359375,
      "learning_rate": 3.66120218579235e-05,
      "loss": 2.2382,
      "step": 490
    },
    {
      "epoch": 2.6885245901639343,
      "grad_norm": 17.92935562133789,
      "learning_rate": 3.655737704918033e-05,
      "loss": 2.1676,
      "step": 492
    },
    {
      "epoch": 2.699453551912568,
      "grad_norm": 16.89613151550293,
      "learning_rate": 3.650273224043716e-05,
      "loss": 1.8069,
      "step": 494
    },
    {
      "epoch": 2.710382513661202,
      "grad_norm": 15.318143844604492,
      "learning_rate": 3.6448087431693986e-05,
      "loss": 2.0624,
      "step": 496
    },
    {
      "epoch": 2.721311475409836,
      "grad_norm": 16.165422439575195,
      "learning_rate": 3.6393442622950826e-05,
      "loss": 2.1891,
      "step": 498
    },
    {
      "epoch": 2.73224043715847,
      "grad_norm": 16.437448501586914,
      "learning_rate": 3.633879781420765e-05,
      "loss": 2.0391,
      "step": 500
    },
    {
      "epoch": 2.7431693989071038,
      "grad_norm": 14.349873542785645,
      "learning_rate": 3.6284153005464486e-05,
      "loss": 1.7852,
      "step": 502
    },
    {
      "epoch": 2.7540983606557377,
      "grad_norm": 15.720614433288574,
      "learning_rate": 3.622950819672131e-05,
      "loss": 1.7678,
      "step": 504
    },
    {
      "epoch": 2.7650273224043715,
      "grad_norm": 13.30111312866211,
      "learning_rate": 3.617486338797814e-05,
      "loss": 1.8655,
      "step": 506
    },
    {
      "epoch": 2.7759562841530054,
      "grad_norm": 17.170299530029297,
      "learning_rate": 3.612021857923498e-05,
      "loss": 2.351,
      "step": 508
    },
    {
      "epoch": 2.7868852459016393,
      "grad_norm": 17.255596160888672,
      "learning_rate": 3.6065573770491806e-05,
      "loss": 2.2175,
      "step": 510
    },
    {
      "epoch": 2.797814207650273,
      "grad_norm": 14.411191940307617,
      "learning_rate": 3.601092896174863e-05,
      "loss": 1.6537,
      "step": 512
    },
    {
      "epoch": 2.808743169398907,
      "grad_norm": 12.936081886291504,
      "learning_rate": 3.5956284153005466e-05,
      "loss": 1.4944,
      "step": 514
    },
    {
      "epoch": 2.819672131147541,
      "grad_norm": 16.61532974243164,
      "learning_rate": 3.590163934426229e-05,
      "loss": 2.4733,
      "step": 516
    },
    {
      "epoch": 2.830601092896175,
      "grad_norm": 17.886737823486328,
      "learning_rate": 3.5846994535519126e-05,
      "loss": 1.9664,
      "step": 518
    },
    {
      "epoch": 2.841530054644809,
      "grad_norm": 15.799398422241211,
      "learning_rate": 3.579234972677596e-05,
      "loss": 2.1535,
      "step": 520
    },
    {
      "epoch": 2.8524590163934427,
      "grad_norm": 16.856712341308594,
      "learning_rate": 3.5737704918032786e-05,
      "loss": 2.1029,
      "step": 522
    },
    {
      "epoch": 2.8633879781420766,
      "grad_norm": 15.608070373535156,
      "learning_rate": 3.568306010928962e-05,
      "loss": 1.8629,
      "step": 524
    },
    {
      "epoch": 2.8743169398907105,
      "grad_norm": 18.747756958007812,
      "learning_rate": 3.5628415300546446e-05,
      "loss": 2.2706,
      "step": 526
    },
    {
      "epoch": 2.8852459016393444,
      "grad_norm": 16.44297981262207,
      "learning_rate": 3.557377049180328e-05,
      "loss": 1.9326,
      "step": 528
    },
    {
      "epoch": 2.8961748633879782,
      "grad_norm": 14.491227149963379,
      "learning_rate": 3.551912568306011e-05,
      "loss": 1.9992,
      "step": 530
    },
    {
      "epoch": 2.907103825136612,
      "grad_norm": 17.893232345581055,
      "learning_rate": 3.546448087431694e-05,
      "loss": 1.7961,
      "step": 532
    },
    {
      "epoch": 2.918032786885246,
      "grad_norm": 16.013652801513672,
      "learning_rate": 3.5409836065573773e-05,
      "loss": 1.9534,
      "step": 534
    },
    {
      "epoch": 2.92896174863388,
      "grad_norm": 13.492508888244629,
      "learning_rate": 3.535519125683061e-05,
      "loss": 2.0244,
      "step": 536
    },
    {
      "epoch": 2.939890710382514,
      "grad_norm": 15.696111679077148,
      "learning_rate": 3.5300546448087433e-05,
      "loss": 1.8291,
      "step": 538
    },
    {
      "epoch": 2.9508196721311473,
      "grad_norm": 13.339873313903809,
      "learning_rate": 3.524590163934427e-05,
      "loss": 1.905,
      "step": 540
    },
    {
      "epoch": 2.9617486338797816,
      "grad_norm": 16.52627944946289,
      "learning_rate": 3.5191256830601094e-05,
      "loss": 2.0683,
      "step": 542
    },
    {
      "epoch": 2.972677595628415,
      "grad_norm": 16.3966121673584,
      "learning_rate": 3.513661202185792e-05,
      "loss": 1.7704,
      "step": 544
    },
    {
      "epoch": 2.9836065573770494,
      "grad_norm": 17.290800094604492,
      "learning_rate": 3.508196721311476e-05,
      "loss": 1.966,
      "step": 546
    },
    {
      "epoch": 2.994535519125683,
      "grad_norm": 16.676353454589844,
      "learning_rate": 3.502732240437159e-05,
      "loss": 2.2085,
      "step": 548
    },
    {
      "epoch": 3.0054644808743167,
      "grad_norm": 16.9572696685791,
      "learning_rate": 3.4972677595628414e-05,
      "loss": 2.0454,
      "step": 550
    },
    {
      "epoch": 3.0163934426229506,
      "grad_norm": 13.841611862182617,
      "learning_rate": 3.491803278688525e-05,
      "loss": 1.7586,
      "step": 552
    },
    {
      "epoch": 3.0273224043715845,
      "grad_norm": 16.11170196533203,
      "learning_rate": 3.4863387978142074e-05,
      "loss": 1.5286,
      "step": 554
    },
    {
      "epoch": 3.0382513661202184,
      "grad_norm": 14.693572998046875,
      "learning_rate": 3.4808743169398914e-05,
      "loss": 1.7293,
      "step": 556
    },
    {
      "epoch": 3.0491803278688523,
      "grad_norm": 12.48723030090332,
      "learning_rate": 3.475409836065574e-05,
      "loss": 1.6187,
      "step": 558
    },
    {
      "epoch": 3.060109289617486,
      "grad_norm": 12.412181854248047,
      "learning_rate": 3.469945355191257e-05,
      "loss": 1.5031,
      "step": 560
    },
    {
      "epoch": 3.07103825136612,
      "grad_norm": 15.588014602661133,
      "learning_rate": 3.46448087431694e-05,
      "loss": 1.8481,
      "step": 562
    },
    {
      "epoch": 3.081967213114754,
      "grad_norm": 17.04337501525879,
      "learning_rate": 3.459016393442623e-05,
      "loss": 1.9091,
      "step": 564
    },
    {
      "epoch": 3.092896174863388,
      "grad_norm": 13.132753372192383,
      "learning_rate": 3.453551912568306e-05,
      "loss": 1.7614,
      "step": 566
    },
    {
      "epoch": 3.1038251366120218,
      "grad_norm": 16.8876895904541,
      "learning_rate": 3.4480874316939894e-05,
      "loss": 1.8796,
      "step": 568
    },
    {
      "epoch": 3.1147540983606556,
      "grad_norm": 17.97928237915039,
      "learning_rate": 3.442622950819672e-05,
      "loss": 1.9075,
      "step": 570
    },
    {
      "epoch": 3.1256830601092895,
      "grad_norm": 17.547470092773438,
      "learning_rate": 3.4371584699453554e-05,
      "loss": 1.7298,
      "step": 572
    },
    {
      "epoch": 3.1366120218579234,
      "grad_norm": 16.938905715942383,
      "learning_rate": 3.431693989071039e-05,
      "loss": 1.8346,
      "step": 574
    },
    {
      "epoch": 3.1475409836065573,
      "grad_norm": 15.997889518737793,
      "learning_rate": 3.4262295081967214e-05,
      "loss": 1.7405,
      "step": 576
    },
    {
      "epoch": 3.158469945355191,
      "grad_norm": 15.072184562683105,
      "learning_rate": 3.420765027322405e-05,
      "loss": 1.7016,
      "step": 578
    },
    {
      "epoch": 3.169398907103825,
      "grad_norm": 15.792972564697266,
      "learning_rate": 3.4153005464480874e-05,
      "loss": 1.7148,
      "step": 580
    },
    {
      "epoch": 3.180327868852459,
      "grad_norm": 15.402606964111328,
      "learning_rate": 3.409836065573771e-05,
      "loss": 1.8803,
      "step": 582
    },
    {
      "epoch": 3.191256830601093,
      "grad_norm": 15.658102989196777,
      "learning_rate": 3.404371584699454e-05,
      "loss": 2.0165,
      "step": 584
    },
    {
      "epoch": 3.202185792349727,
      "grad_norm": 15.837913513183594,
      "learning_rate": 3.398907103825137e-05,
      "loss": 1.9193,
      "step": 586
    },
    {
      "epoch": 3.2131147540983607,
      "grad_norm": 12.726523399353027,
      "learning_rate": 3.39344262295082e-05,
      "loss": 1.6996,
      "step": 588
    },
    {
      "epoch": 3.2240437158469946,
      "grad_norm": 15.310430526733398,
      "learning_rate": 3.387978142076503e-05,
      "loss": 1.4137,
      "step": 590
    },
    {
      "epoch": 3.2349726775956285,
      "grad_norm": 13.833639144897461,
      "learning_rate": 3.3825136612021854e-05,
      "loss": 1.3277,
      "step": 592
    },
    {
      "epoch": 3.2459016393442623,
      "grad_norm": 16.74729347229004,
      "learning_rate": 3.3770491803278695e-05,
      "loss": 1.3925,
      "step": 594
    },
    {
      "epoch": 3.2568306010928962,
      "grad_norm": 12.845205307006836,
      "learning_rate": 3.371584699453552e-05,
      "loss": 1.2334,
      "step": 596
    },
    {
      "epoch": 3.26775956284153,
      "grad_norm": 13.230401039123535,
      "learning_rate": 3.366120218579235e-05,
      "loss": 1.3353,
      "step": 598
    },
    {
      "epoch": 3.278688524590164,
      "grad_norm": 14.736532211303711,
      "learning_rate": 3.360655737704918e-05,
      "loss": 1.5391,
      "step": 600
    },
    {
      "epoch": 3.289617486338798,
      "grad_norm": 17.5513916015625,
      "learning_rate": 3.355191256830601e-05,
      "loss": 1.6294,
      "step": 602
    },
    {
      "epoch": 3.300546448087432,
      "grad_norm": 17.22604751586914,
      "learning_rate": 3.349726775956284e-05,
      "loss": 1.8728,
      "step": 604
    },
    {
      "epoch": 3.3114754098360657,
      "grad_norm": 13.434248924255371,
      "learning_rate": 3.3442622950819675e-05,
      "loss": 1.542,
      "step": 606
    },
    {
      "epoch": 3.3224043715846996,
      "grad_norm": 17.101985931396484,
      "learning_rate": 3.33879781420765e-05,
      "loss": 1.8183,
      "step": 608
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 15.304283142089844,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 1.9613,
      "step": 610
    },
    {
      "epoch": 3.3442622950819674,
      "grad_norm": 18.401609420776367,
      "learning_rate": 3.327868852459017e-05,
      "loss": 1.9695,
      "step": 612
    },
    {
      "epoch": 3.3551912568306013,
      "grad_norm": 14.951178550720215,
      "learning_rate": 3.3224043715846995e-05,
      "loss": 1.7026,
      "step": 614
    },
    {
      "epoch": 3.366120218579235,
      "grad_norm": 15.29007625579834,
      "learning_rate": 3.316939890710383e-05,
      "loss": 2.0253,
      "step": 616
    },
    {
      "epoch": 3.3770491803278686,
      "grad_norm": 14.861567497253418,
      "learning_rate": 3.3114754098360655e-05,
      "loss": 1.7501,
      "step": 618
    },
    {
      "epoch": 3.387978142076503,
      "grad_norm": 16.398256301879883,
      "learning_rate": 3.306010928961749e-05,
      "loss": 2.1659,
      "step": 620
    },
    {
      "epoch": 3.3989071038251364,
      "grad_norm": 16.054353713989258,
      "learning_rate": 3.300546448087432e-05,
      "loss": 1.9073,
      "step": 622
    },
    {
      "epoch": 3.4098360655737707,
      "grad_norm": 16.86777687072754,
      "learning_rate": 3.295081967213115e-05,
      "loss": 2.0064,
      "step": 624
    },
    {
      "epoch": 3.420765027322404,
      "grad_norm": 16.77975082397461,
      "learning_rate": 3.289617486338798e-05,
      "loss": 1.7726,
      "step": 626
    },
    {
      "epoch": 3.431693989071038,
      "grad_norm": 16.90477752685547,
      "learning_rate": 3.284153005464481e-05,
      "loss": 1.9402,
      "step": 628
    },
    {
      "epoch": 3.442622950819672,
      "grad_norm": 13.208745002746582,
      "learning_rate": 3.2786885245901635e-05,
      "loss": 1.551,
      "step": 630
    },
    {
      "epoch": 3.453551912568306,
      "grad_norm": 17.102630615234375,
      "learning_rate": 3.2732240437158475e-05,
      "loss": 1.9863,
      "step": 632
    },
    {
      "epoch": 3.4644808743169397,
      "grad_norm": 13.410622596740723,
      "learning_rate": 3.26775956284153e-05,
      "loss": 1.5241,
      "step": 634
    },
    {
      "epoch": 3.4754098360655736,
      "grad_norm": 15.272748947143555,
      "learning_rate": 3.2622950819672136e-05,
      "loss": 1.7757,
      "step": 636
    },
    {
      "epoch": 3.4863387978142075,
      "grad_norm": 12.894949913024902,
      "learning_rate": 3.256830601092896e-05,
      "loss": 1.6568,
      "step": 638
    },
    {
      "epoch": 3.4972677595628414,
      "grad_norm": 17.294212341308594,
      "learning_rate": 3.251366120218579e-05,
      "loss": 1.9206,
      "step": 640
    },
    {
      "epoch": 3.5081967213114753,
      "grad_norm": 16.261884689331055,
      "learning_rate": 3.245901639344263e-05,
      "loss": 1.7091,
      "step": 642
    },
    {
      "epoch": 3.519125683060109,
      "grad_norm": 16.071922302246094,
      "learning_rate": 3.2404371584699456e-05,
      "loss": 2.1365,
      "step": 644
    },
    {
      "epoch": 3.530054644808743,
      "grad_norm": 14.259743690490723,
      "learning_rate": 3.234972677595628e-05,
      "loss": 1.5905,
      "step": 646
    },
    {
      "epoch": 3.540983606557377,
      "grad_norm": 17.374725341796875,
      "learning_rate": 3.2295081967213116e-05,
      "loss": 1.9034,
      "step": 648
    },
    {
      "epoch": 3.551912568306011,
      "grad_norm": 12.997698783874512,
      "learning_rate": 3.224043715846995e-05,
      "loss": 1.5387,
      "step": 650
    },
    {
      "epoch": 3.5628415300546448,
      "grad_norm": 17.221492767333984,
      "learning_rate": 3.2185792349726776e-05,
      "loss": 1.9571,
      "step": 652
    },
    {
      "epoch": 3.5737704918032787,
      "grad_norm": 15.995098114013672,
      "learning_rate": 3.213114754098361e-05,
      "loss": 1.7693,
      "step": 654
    },
    {
      "epoch": 3.5846994535519126,
      "grad_norm": 15.350930213928223,
      "learning_rate": 3.2076502732240436e-05,
      "loss": 1.5408,
      "step": 656
    },
    {
      "epoch": 3.5956284153005464,
      "grad_norm": 13.622849464416504,
      "learning_rate": 3.202185792349727e-05,
      "loss": 1.7047,
      "step": 658
    },
    {
      "epoch": 3.6065573770491803,
      "grad_norm": 14.299799919128418,
      "learning_rate": 3.19672131147541e-05,
      "loss": 1.5525,
      "step": 660
    },
    {
      "epoch": 3.6174863387978142,
      "grad_norm": 12.514612197875977,
      "learning_rate": 3.191256830601093e-05,
      "loss": 1.4957,
      "step": 662
    },
    {
      "epoch": 3.628415300546448,
      "grad_norm": 16.261497497558594,
      "learning_rate": 3.185792349726776e-05,
      "loss": 1.5959,
      "step": 664
    },
    {
      "epoch": 3.639344262295082,
      "grad_norm": 15.689848899841309,
      "learning_rate": 3.180327868852459e-05,
      "loss": 1.7298,
      "step": 666
    },
    {
      "epoch": 3.650273224043716,
      "grad_norm": 15.240104675292969,
      "learning_rate": 3.174863387978142e-05,
      "loss": 1.7693,
      "step": 668
    },
    {
      "epoch": 3.66120218579235,
      "grad_norm": 15.002880096435547,
      "learning_rate": 3.1693989071038256e-05,
      "loss": 1.9375,
      "step": 670
    },
    {
      "epoch": 3.6721311475409837,
      "grad_norm": 16.227201461791992,
      "learning_rate": 3.163934426229508e-05,
      "loss": 1.9646,
      "step": 672
    },
    {
      "epoch": 3.6830601092896176,
      "grad_norm": 14.672246932983398,
      "learning_rate": 3.1584699453551916e-05,
      "loss": 1.8465,
      "step": 674
    },
    {
      "epoch": 3.6939890710382515,
      "grad_norm": 15.66124439239502,
      "learning_rate": 3.153005464480874e-05,
      "loss": 1.7137,
      "step": 676
    },
    {
      "epoch": 3.7049180327868854,
      "grad_norm": 17.76232147216797,
      "learning_rate": 3.1475409836065576e-05,
      "loss": 2.0789,
      "step": 678
    },
    {
      "epoch": 3.7158469945355193,
      "grad_norm": 12.495217323303223,
      "learning_rate": 3.142076502732241e-05,
      "loss": 1.5678,
      "step": 680
    },
    {
      "epoch": 3.726775956284153,
      "grad_norm": 16.56146812438965,
      "learning_rate": 3.1366120218579236e-05,
      "loss": 1.8284,
      "step": 682
    },
    {
      "epoch": 3.737704918032787,
      "grad_norm": 15.313247680664062,
      "learning_rate": 3.131147540983606e-05,
      "loss": 1.7784,
      "step": 684
    },
    {
      "epoch": 3.748633879781421,
      "grad_norm": 12.505980491638184,
      "learning_rate": 3.1256830601092896e-05,
      "loss": 1.3133,
      "step": 686
    },
    {
      "epoch": 3.7595628415300544,
      "grad_norm": 14.954073905944824,
      "learning_rate": 3.120218579234973e-05,
      "loss": 1.7985,
      "step": 688
    },
    {
      "epoch": 3.7704918032786887,
      "grad_norm": 16.877079010009766,
      "learning_rate": 3.114754098360656e-05,
      "loss": 1.9422,
      "step": 690
    },
    {
      "epoch": 3.781420765027322,
      "grad_norm": 14.265820503234863,
      "learning_rate": 3.109289617486339e-05,
      "loss": 1.5905,
      "step": 692
    },
    {
      "epoch": 3.7923497267759565,
      "grad_norm": 16.72311782836914,
      "learning_rate": 3.1038251366120217e-05,
      "loss": 1.7471,
      "step": 694
    },
    {
      "epoch": 3.80327868852459,
      "grad_norm": 13.592775344848633,
      "learning_rate": 3.098360655737705e-05,
      "loss": 1.7092,
      "step": 696
    },
    {
      "epoch": 3.8142076502732243,
      "grad_norm": 16.20517349243164,
      "learning_rate": 3.0928961748633883e-05,
      "loss": 1.6622,
      "step": 698
    },
    {
      "epoch": 3.8251366120218577,
      "grad_norm": 12.69473648071289,
      "learning_rate": 3.087431693989071e-05,
      "loss": 1.5693,
      "step": 700
    },
    {
      "epoch": 3.836065573770492,
      "grad_norm": 13.371788024902344,
      "learning_rate": 3.0819672131147544e-05,
      "loss": 1.7413,
      "step": 702
    },
    {
      "epoch": 3.8469945355191255,
      "grad_norm": 12.62033748626709,
      "learning_rate": 3.076502732240437e-05,
      "loss": 1.5021,
      "step": 704
    },
    {
      "epoch": 3.8579234972677594,
      "grad_norm": 15.373640060424805,
      "learning_rate": 3.0710382513661204e-05,
      "loss": 1.6486,
      "step": 706
    },
    {
      "epoch": 3.8688524590163933,
      "grad_norm": 19.54924774169922,
      "learning_rate": 3.065573770491804e-05,
      "loss": 1.7149,
      "step": 708
    },
    {
      "epoch": 3.879781420765027,
      "grad_norm": 16.346345901489258,
      "learning_rate": 3.0601092896174864e-05,
      "loss": 1.767,
      "step": 710
    },
    {
      "epoch": 3.890710382513661,
      "grad_norm": 15.661288261413574,
      "learning_rate": 3.05464480874317e-05,
      "loss": 2.0976,
      "step": 712
    },
    {
      "epoch": 3.901639344262295,
      "grad_norm": 12.440030097961426,
      "learning_rate": 3.0491803278688524e-05,
      "loss": 1.3139,
      "step": 714
    },
    {
      "epoch": 3.912568306010929,
      "grad_norm": 14.417861938476562,
      "learning_rate": 3.043715846994536e-05,
      "loss": 1.5777,
      "step": 716
    },
    {
      "epoch": 3.9234972677595628,
      "grad_norm": 14.354331970214844,
      "learning_rate": 3.0382513661202187e-05,
      "loss": 1.858,
      "step": 718
    },
    {
      "epoch": 3.9344262295081966,
      "grad_norm": 15.663443565368652,
      "learning_rate": 3.0327868852459017e-05,
      "loss": 1.6555,
      "step": 720
    },
    {
      "epoch": 3.9453551912568305,
      "grad_norm": 13.898241996765137,
      "learning_rate": 3.0273224043715847e-05,
      "loss": 1.6794,
      "step": 722
    },
    {
      "epoch": 3.9562841530054644,
      "grad_norm": 11.946357727050781,
      "learning_rate": 3.0218579234972677e-05,
      "loss": 1.4583,
      "step": 724
    },
    {
      "epoch": 3.9672131147540983,
      "grad_norm": 14.415071487426758,
      "learning_rate": 3.016393442622951e-05,
      "loss": 1.65,
      "step": 726
    },
    {
      "epoch": 3.978142076502732,
      "grad_norm": 15.728798866271973,
      "learning_rate": 3.010928961748634e-05,
      "loss": 1.9247,
      "step": 728
    },
    {
      "epoch": 3.989071038251366,
      "grad_norm": 12.70474910736084,
      "learning_rate": 3.005464480874317e-05,
      "loss": 1.3972,
      "step": 730
    },
    {
      "epoch": 4.0,
      "grad_norm": 15.04383659362793,
      "learning_rate": 3e-05,
      "loss": 1.9294,
      "step": 732
    },
    {
      "epoch": 4.0109289617486334,
      "grad_norm": 12.466110229492188,
      "learning_rate": 2.994535519125683e-05,
      "loss": 1.2151,
      "step": 734
    },
    {
      "epoch": 4.021857923497268,
      "grad_norm": 14.839356422424316,
      "learning_rate": 2.9890710382513664e-05,
      "loss": 1.1851,
      "step": 736
    },
    {
      "epoch": 4.032786885245901,
      "grad_norm": 16.60171127319336,
      "learning_rate": 2.9836065573770494e-05,
      "loss": 1.828,
      "step": 738
    },
    {
      "epoch": 4.043715846994536,
      "grad_norm": 15.468013763427734,
      "learning_rate": 2.9781420765027324e-05,
      "loss": 1.3265,
      "step": 740
    },
    {
      "epoch": 4.054644808743169,
      "grad_norm": 11.761736869812012,
      "learning_rate": 2.9726775956284154e-05,
      "loss": 1.3567,
      "step": 742
    },
    {
      "epoch": 4.065573770491803,
      "grad_norm": 12.461751937866211,
      "learning_rate": 2.967213114754098e-05,
      "loss": 1.1001,
      "step": 744
    },
    {
      "epoch": 4.076502732240437,
      "grad_norm": 13.467342376708984,
      "learning_rate": 2.9617486338797818e-05,
      "loss": 1.5901,
      "step": 746
    },
    {
      "epoch": 4.087431693989071,
      "grad_norm": 15.048318862915039,
      "learning_rate": 2.9562841530054648e-05,
      "loss": 1.6453,
      "step": 748
    },
    {
      "epoch": 4.098360655737705,
      "grad_norm": 16.15920639038086,
      "learning_rate": 2.9508196721311478e-05,
      "loss": 1.3893,
      "step": 750
    },
    {
      "epoch": 4.109289617486339,
      "grad_norm": 15.106892585754395,
      "learning_rate": 2.9453551912568304e-05,
      "loss": 1.4172,
      "step": 752
    },
    {
      "epoch": 4.120218579234972,
      "grad_norm": 14.308136940002441,
      "learning_rate": 2.939890710382514e-05,
      "loss": 1.5744,
      "step": 754
    },
    {
      "epoch": 4.131147540983607,
      "grad_norm": 13.082691192626953,
      "learning_rate": 2.934426229508197e-05,
      "loss": 1.2824,
      "step": 756
    },
    {
      "epoch": 4.14207650273224,
      "grad_norm": 15.43599796295166,
      "learning_rate": 2.9289617486338798e-05,
      "loss": 1.6196,
      "step": 758
    },
    {
      "epoch": 4.1530054644808745,
      "grad_norm": 12.37364673614502,
      "learning_rate": 2.9234972677595628e-05,
      "loss": 1.4383,
      "step": 760
    },
    {
      "epoch": 4.163934426229508,
      "grad_norm": 16.42101287841797,
      "learning_rate": 2.9180327868852458e-05,
      "loss": 1.5838,
      "step": 762
    },
    {
      "epoch": 4.174863387978142,
      "grad_norm": 15.34058952331543,
      "learning_rate": 2.9125683060109295e-05,
      "loss": 1.3734,
      "step": 764
    },
    {
      "epoch": 4.185792349726776,
      "grad_norm": 12.075984954833984,
      "learning_rate": 2.907103825136612e-05,
      "loss": 1.1986,
      "step": 766
    },
    {
      "epoch": 4.19672131147541,
      "grad_norm": 14.153106689453125,
      "learning_rate": 2.901639344262295e-05,
      "loss": 1.1407,
      "step": 768
    },
    {
      "epoch": 4.2076502732240435,
      "grad_norm": 12.33275032043457,
      "learning_rate": 2.896174863387978e-05,
      "loss": 1.3065,
      "step": 770
    },
    {
      "epoch": 4.218579234972678,
      "grad_norm": 15.644842147827148,
      "learning_rate": 2.890710382513661e-05,
      "loss": 1.4122,
      "step": 772
    },
    {
      "epoch": 4.229508196721311,
      "grad_norm": 15.6301908493042,
      "learning_rate": 2.8852459016393445e-05,
      "loss": 1.4684,
      "step": 774
    },
    {
      "epoch": 4.240437158469946,
      "grad_norm": 13.152438163757324,
      "learning_rate": 2.8797814207650275e-05,
      "loss": 1.2545,
      "step": 776
    },
    {
      "epoch": 4.251366120218579,
      "grad_norm": 14.705275535583496,
      "learning_rate": 2.8743169398907105e-05,
      "loss": 1.4357,
      "step": 778
    },
    {
      "epoch": 4.262295081967213,
      "grad_norm": 14.496620178222656,
      "learning_rate": 2.8688524590163935e-05,
      "loss": 1.3082,
      "step": 780
    },
    {
      "epoch": 4.273224043715847,
      "grad_norm": 15.362818717956543,
      "learning_rate": 2.8633879781420765e-05,
      "loss": 1.5134,
      "step": 782
    },
    {
      "epoch": 4.284153005464481,
      "grad_norm": 16.88199234008789,
      "learning_rate": 2.85792349726776e-05,
      "loss": 1.9378,
      "step": 784
    },
    {
      "epoch": 4.295081967213115,
      "grad_norm": 14.16037654876709,
      "learning_rate": 2.852459016393443e-05,
      "loss": 1.6035,
      "step": 786
    },
    {
      "epoch": 4.306010928961749,
      "grad_norm": 12.15011978149414,
      "learning_rate": 2.846994535519126e-05,
      "loss": 1.168,
      "step": 788
    },
    {
      "epoch": 4.316939890710382,
      "grad_norm": 12.994043350219727,
      "learning_rate": 2.841530054644809e-05,
      "loss": 1.3802,
      "step": 790
    },
    {
      "epoch": 4.327868852459017,
      "grad_norm": 15.069378852844238,
      "learning_rate": 2.8360655737704922e-05,
      "loss": 1.6914,
      "step": 792
    },
    {
      "epoch": 4.33879781420765,
      "grad_norm": 13.110207557678223,
      "learning_rate": 2.8306010928961752e-05,
      "loss": 1.4153,
      "step": 794
    },
    {
      "epoch": 4.3497267759562845,
      "grad_norm": 18.363332748413086,
      "learning_rate": 2.8251366120218582e-05,
      "loss": 1.7148,
      "step": 796
    },
    {
      "epoch": 4.360655737704918,
      "grad_norm": 15.937100410461426,
      "learning_rate": 2.819672131147541e-05,
      "loss": 1.7958,
      "step": 798
    },
    {
      "epoch": 4.371584699453552,
      "grad_norm": 15.007532119750977,
      "learning_rate": 2.814207650273224e-05,
      "loss": 1.6621,
      "step": 800
    },
    {
      "epoch": 4.382513661202186,
      "grad_norm": 12.200531005859375,
      "learning_rate": 2.8087431693989076e-05,
      "loss": 1.3778,
      "step": 802
    },
    {
      "epoch": 4.39344262295082,
      "grad_norm": 12.535780906677246,
      "learning_rate": 2.8032786885245906e-05,
      "loss": 1.1622,
      "step": 804
    },
    {
      "epoch": 4.404371584699454,
      "grad_norm": 12.515442848205566,
      "learning_rate": 2.7978142076502732e-05,
      "loss": 1.434,
      "step": 806
    },
    {
      "epoch": 4.415300546448087,
      "grad_norm": 13.788564682006836,
      "learning_rate": 2.7923497267759562e-05,
      "loss": 1.4449,
      "step": 808
    },
    {
      "epoch": 4.426229508196721,
      "grad_norm": 15.711039543151855,
      "learning_rate": 2.7868852459016392e-05,
      "loss": 1.6788,
      "step": 810
    },
    {
      "epoch": 4.437158469945355,
      "grad_norm": 14.83806037902832,
      "learning_rate": 2.7814207650273226e-05,
      "loss": 1.7801,
      "step": 812
    },
    {
      "epoch": 4.448087431693989,
      "grad_norm": 14.273966789245605,
      "learning_rate": 2.7759562841530056e-05,
      "loss": 1.3115,
      "step": 814
    },
    {
      "epoch": 4.459016393442623,
      "grad_norm": 16.787702560424805,
      "learning_rate": 2.7704918032786886e-05,
      "loss": 1.6746,
      "step": 816
    },
    {
      "epoch": 4.469945355191257,
      "grad_norm": 17.266632080078125,
      "learning_rate": 2.7650273224043716e-05,
      "loss": 1.5378,
      "step": 818
    },
    {
      "epoch": 4.48087431693989,
      "grad_norm": 15.501355171203613,
      "learning_rate": 2.7595628415300546e-05,
      "loss": 1.4246,
      "step": 820
    },
    {
      "epoch": 4.491803278688525,
      "grad_norm": 15.628273010253906,
      "learning_rate": 2.754098360655738e-05,
      "loss": 1.5504,
      "step": 822
    },
    {
      "epoch": 4.502732240437158,
      "grad_norm": 16.299745559692383,
      "learning_rate": 2.748633879781421e-05,
      "loss": 1.5047,
      "step": 824
    },
    {
      "epoch": 4.5136612021857925,
      "grad_norm": 16.094501495361328,
      "learning_rate": 2.743169398907104e-05,
      "loss": 1.5788,
      "step": 826
    },
    {
      "epoch": 4.524590163934426,
      "grad_norm": 15.924139022827148,
      "learning_rate": 2.737704918032787e-05,
      "loss": 1.549,
      "step": 828
    },
    {
      "epoch": 4.53551912568306,
      "grad_norm": 16.108478546142578,
      "learning_rate": 2.7322404371584703e-05,
      "loss": 1.6474,
      "step": 830
    },
    {
      "epoch": 4.546448087431694,
      "grad_norm": 15.482409477233887,
      "learning_rate": 2.7267759562841533e-05,
      "loss": 1.5781,
      "step": 832
    },
    {
      "epoch": 4.557377049180328,
      "grad_norm": 15.701160430908203,
      "learning_rate": 2.7213114754098363e-05,
      "loss": 1.8545,
      "step": 834
    },
    {
      "epoch": 4.5683060109289615,
      "grad_norm": 16.117542266845703,
      "learning_rate": 2.7158469945355193e-05,
      "loss": 1.4646,
      "step": 836
    },
    {
      "epoch": 4.579234972677596,
      "grad_norm": 13.435876846313477,
      "learning_rate": 2.710382513661202e-05,
      "loss": 1.4845,
      "step": 838
    },
    {
      "epoch": 4.590163934426229,
      "grad_norm": 16.809783935546875,
      "learning_rate": 2.7049180327868856e-05,
      "loss": 1.4775,
      "step": 840
    },
    {
      "epoch": 4.601092896174864,
      "grad_norm": 16.00694465637207,
      "learning_rate": 2.6994535519125686e-05,
      "loss": 1.5562,
      "step": 842
    },
    {
      "epoch": 4.612021857923497,
      "grad_norm": 15.608155250549316,
      "learning_rate": 2.6939890710382516e-05,
      "loss": 1.6317,
      "step": 844
    },
    {
      "epoch": 4.622950819672131,
      "grad_norm": 15.530098915100098,
      "learning_rate": 2.6885245901639343e-05,
      "loss": 1.3427,
      "step": 846
    },
    {
      "epoch": 4.633879781420765,
      "grad_norm": 15.7657470703125,
      "learning_rate": 2.6830601092896173e-05,
      "loss": 1.6872,
      "step": 848
    },
    {
      "epoch": 4.644808743169399,
      "grad_norm": 13.203641891479492,
      "learning_rate": 2.677595628415301e-05,
      "loss": 1.1717,
      "step": 850
    },
    {
      "epoch": 4.655737704918033,
      "grad_norm": 12.977883338928223,
      "learning_rate": 2.6721311475409837e-05,
      "loss": 1.4154,
      "step": 852
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 16.382863998413086,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 1.42,
      "step": 854
    },
    {
      "epoch": 4.6775956284153,
      "grad_norm": 15.51695728302002,
      "learning_rate": 2.6612021857923497e-05,
      "loss": 1.6587,
      "step": 856
    },
    {
      "epoch": 4.688524590163935,
      "grad_norm": 15.452481269836426,
      "learning_rate": 2.6557377049180327e-05,
      "loss": 1.5879,
      "step": 858
    },
    {
      "epoch": 4.699453551912568,
      "grad_norm": 12.2123441696167,
      "learning_rate": 2.650273224043716e-05,
      "loss": 1.5008,
      "step": 860
    },
    {
      "epoch": 4.7103825136612025,
      "grad_norm": 16.72183609008789,
      "learning_rate": 2.644808743169399e-05,
      "loss": 1.3314,
      "step": 862
    },
    {
      "epoch": 4.721311475409836,
      "grad_norm": 16.595504760742188,
      "learning_rate": 2.639344262295082e-05,
      "loss": 1.3799,
      "step": 864
    },
    {
      "epoch": 4.73224043715847,
      "grad_norm": 14.880974769592285,
      "learning_rate": 2.633879781420765e-05,
      "loss": 1.3871,
      "step": 866
    },
    {
      "epoch": 4.743169398907104,
      "grad_norm": 16.059635162353516,
      "learning_rate": 2.6284153005464484e-05,
      "loss": 1.5442,
      "step": 868
    },
    {
      "epoch": 4.754098360655737,
      "grad_norm": 12.755610466003418,
      "learning_rate": 2.6229508196721314e-05,
      "loss": 1.4022,
      "step": 870
    },
    {
      "epoch": 4.7650273224043715,
      "grad_norm": 16.507545471191406,
      "learning_rate": 2.6174863387978144e-05,
      "loss": 1.6471,
      "step": 872
    },
    {
      "epoch": 4.775956284153006,
      "grad_norm": 13.075278282165527,
      "learning_rate": 2.6120218579234974e-05,
      "loss": 1.4164,
      "step": 874
    },
    {
      "epoch": 4.786885245901639,
      "grad_norm": 15.42963981628418,
      "learning_rate": 2.6065573770491804e-05,
      "loss": 1.8558,
      "step": 876
    },
    {
      "epoch": 4.797814207650273,
      "grad_norm": 13.282312393188477,
      "learning_rate": 2.6010928961748637e-05,
      "loss": 1.0627,
      "step": 878
    },
    {
      "epoch": 4.808743169398907,
      "grad_norm": 15.175551414489746,
      "learning_rate": 2.5956284153005467e-05,
      "loss": 1.6396,
      "step": 880
    },
    {
      "epoch": 4.8196721311475414,
      "grad_norm": 16.589292526245117,
      "learning_rate": 2.5901639344262297e-05,
      "loss": 1.7768,
      "step": 882
    },
    {
      "epoch": 4.830601092896175,
      "grad_norm": 15.552449226379395,
      "learning_rate": 2.5846994535519127e-05,
      "loss": 1.4022,
      "step": 884
    },
    {
      "epoch": 4.841530054644808,
      "grad_norm": 13.874085426330566,
      "learning_rate": 2.5792349726775954e-05,
      "loss": 1.5088,
      "step": 886
    },
    {
      "epoch": 4.852459016393443,
      "grad_norm": 16.215913772583008,
      "learning_rate": 2.573770491803279e-05,
      "loss": 1.7344,
      "step": 888
    },
    {
      "epoch": 4.863387978142076,
      "grad_norm": 14.880178451538086,
      "learning_rate": 2.568306010928962e-05,
      "loss": 1.5212,
      "step": 890
    },
    {
      "epoch": 4.8743169398907105,
      "grad_norm": 13.607626914978027,
      "learning_rate": 2.5628415300546447e-05,
      "loss": 1.4639,
      "step": 892
    },
    {
      "epoch": 4.885245901639344,
      "grad_norm": 17.236835479736328,
      "learning_rate": 2.5573770491803277e-05,
      "loss": 1.5261,
      "step": 894
    },
    {
      "epoch": 4.896174863387978,
      "grad_norm": 14.686067581176758,
      "learning_rate": 2.5519125683060107e-05,
      "loss": 1.6178,
      "step": 896
    },
    {
      "epoch": 4.907103825136612,
      "grad_norm": 16.214448928833008,
      "learning_rate": 2.5464480874316944e-05,
      "loss": 1.6866,
      "step": 898
    },
    {
      "epoch": 4.918032786885246,
      "grad_norm": 15.385623931884766,
      "learning_rate": 2.540983606557377e-05,
      "loss": 1.5908,
      "step": 900
    },
    {
      "epoch": 4.9289617486338795,
      "grad_norm": 13.75435733795166,
      "learning_rate": 2.53551912568306e-05,
      "loss": 1.4015,
      "step": 902
    },
    {
      "epoch": 4.939890710382514,
      "grad_norm": 16.0394287109375,
      "learning_rate": 2.530054644808743e-05,
      "loss": 1.7894,
      "step": 904
    },
    {
      "epoch": 4.950819672131147,
      "grad_norm": 15.909063339233398,
      "learning_rate": 2.5245901639344264e-05,
      "loss": 1.443,
      "step": 906
    },
    {
      "epoch": 4.961748633879782,
      "grad_norm": 17.27530860900879,
      "learning_rate": 2.5191256830601094e-05,
      "loss": 1.8593,
      "step": 908
    },
    {
      "epoch": 4.972677595628415,
      "grad_norm": 13.296188354492188,
      "learning_rate": 2.5136612021857924e-05,
      "loss": 1.4096,
      "step": 910
    },
    {
      "epoch": 4.983606557377049,
      "grad_norm": 16.84560775756836,
      "learning_rate": 2.5081967213114754e-05,
      "loss": 1.7435,
      "step": 912
    },
    {
      "epoch": 4.994535519125683,
      "grad_norm": 15.957833290100098,
      "learning_rate": 2.5027322404371584e-05,
      "loss": 1.5748,
      "step": 914
    },
    {
      "epoch": 5.005464480874317,
      "grad_norm": 13.4794340133667,
      "learning_rate": 2.4972677595628415e-05,
      "loss": 1.4273,
      "step": 916
    },
    {
      "epoch": 5.016393442622951,
      "grad_norm": 14.64804744720459,
      "learning_rate": 2.4918032786885248e-05,
      "loss": 1.4388,
      "step": 918
    },
    {
      "epoch": 5.027322404371585,
      "grad_norm": 14.173283576965332,
      "learning_rate": 2.4863387978142078e-05,
      "loss": 1.2166,
      "step": 920
    },
    {
      "epoch": 5.038251366120218,
      "grad_norm": 11.799335479736328,
      "learning_rate": 2.4808743169398908e-05,
      "loss": 1.1022,
      "step": 922
    },
    {
      "epoch": 5.049180327868853,
      "grad_norm": 15.334259986877441,
      "learning_rate": 2.4754098360655738e-05,
      "loss": 1.5268,
      "step": 924
    },
    {
      "epoch": 5.060109289617486,
      "grad_norm": 14.911015510559082,
      "learning_rate": 2.4699453551912568e-05,
      "loss": 1.2353,
      "step": 926
    },
    {
      "epoch": 5.0710382513661205,
      "grad_norm": 12.538288116455078,
      "learning_rate": 2.46448087431694e-05,
      "loss": 1.1825,
      "step": 928
    },
    {
      "epoch": 5.081967213114754,
      "grad_norm": 12.823322296142578,
      "learning_rate": 2.459016393442623e-05,
      "loss": 1.559,
      "step": 930
    },
    {
      "epoch": 5.092896174863388,
      "grad_norm": 12.172510147094727,
      "learning_rate": 2.453551912568306e-05,
      "loss": 1.2307,
      "step": 932
    },
    {
      "epoch": 5.103825136612022,
      "grad_norm": 11.938446998596191,
      "learning_rate": 2.448087431693989e-05,
      "loss": 1.1932,
      "step": 934
    },
    {
      "epoch": 5.114754098360656,
      "grad_norm": 16.824129104614258,
      "learning_rate": 2.442622950819672e-05,
      "loss": 1.2012,
      "step": 936
    },
    {
      "epoch": 5.1256830601092895,
      "grad_norm": 14.202142715454102,
      "learning_rate": 2.4371584699453555e-05,
      "loss": 1.1566,
      "step": 938
    },
    {
      "epoch": 5.136612021857924,
      "grad_norm": 14.6981782913208,
      "learning_rate": 2.431693989071038e-05,
      "loss": 1.4802,
      "step": 940
    },
    {
      "epoch": 5.147540983606557,
      "grad_norm": 12.40621280670166,
      "learning_rate": 2.4262295081967215e-05,
      "loss": 1.276,
      "step": 942
    },
    {
      "epoch": 5.158469945355192,
      "grad_norm": 13.719059944152832,
      "learning_rate": 2.4207650273224045e-05,
      "loss": 1.2652,
      "step": 944
    },
    {
      "epoch": 5.169398907103825,
      "grad_norm": 13.275870323181152,
      "learning_rate": 2.4153005464480875e-05,
      "loss": 1.3292,
      "step": 946
    },
    {
      "epoch": 5.180327868852459,
      "grad_norm": 14.897323608398438,
      "learning_rate": 2.4098360655737705e-05,
      "loss": 1.236,
      "step": 948
    },
    {
      "epoch": 5.191256830601093,
      "grad_norm": 14.85920238494873,
      "learning_rate": 2.4043715846994535e-05,
      "loss": 1.4185,
      "step": 950
    },
    {
      "epoch": 5.202185792349727,
      "grad_norm": 14.490652084350586,
      "learning_rate": 2.398907103825137e-05,
      "loss": 1.0904,
      "step": 952
    },
    {
      "epoch": 5.213114754098361,
      "grad_norm": 11.769539833068848,
      "learning_rate": 2.39344262295082e-05,
      "loss": 1.0101,
      "step": 954
    },
    {
      "epoch": 5.224043715846994,
      "grad_norm": 15.564607620239258,
      "learning_rate": 2.387978142076503e-05,
      "loss": 1.4138,
      "step": 956
    },
    {
      "epoch": 5.2349726775956285,
      "grad_norm": 16.400697708129883,
      "learning_rate": 2.382513661202186e-05,
      "loss": 1.3326,
      "step": 958
    },
    {
      "epoch": 5.245901639344262,
      "grad_norm": 15.6705961227417,
      "learning_rate": 2.377049180327869e-05,
      "loss": 1.5062,
      "step": 960
    },
    {
      "epoch": 5.256830601092896,
      "grad_norm": 15.375910758972168,
      "learning_rate": 2.3715846994535522e-05,
      "loss": 1.2922,
      "step": 962
    },
    {
      "epoch": 5.26775956284153,
      "grad_norm": 18.2454833984375,
      "learning_rate": 2.366120218579235e-05,
      "loss": 1.303,
      "step": 964
    },
    {
      "epoch": 5.278688524590164,
      "grad_norm": 14.687777519226074,
      "learning_rate": 2.3606557377049182e-05,
      "loss": 1.554,
      "step": 966
    },
    {
      "epoch": 5.2896174863387975,
      "grad_norm": 13.28370475769043,
      "learning_rate": 2.3551912568306012e-05,
      "loss": 0.9914,
      "step": 968
    },
    {
      "epoch": 5.300546448087432,
      "grad_norm": 12.49671459197998,
      "learning_rate": 2.3497267759562842e-05,
      "loss": 0.9125,
      "step": 970
    },
    {
      "epoch": 5.311475409836065,
      "grad_norm": 12.301025390625,
      "learning_rate": 2.3442622950819672e-05,
      "loss": 1.1679,
      "step": 972
    },
    {
      "epoch": 5.3224043715847,
      "grad_norm": 14.081159591674805,
      "learning_rate": 2.3387978142076502e-05,
      "loss": 1.223,
      "step": 974
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 15.423940658569336,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 1.3796,
      "step": 976
    },
    {
      "epoch": 5.344262295081967,
      "grad_norm": 15.177644729614258,
      "learning_rate": 2.3278688524590166e-05,
      "loss": 1.4682,
      "step": 978
    },
    {
      "epoch": 5.355191256830601,
      "grad_norm": 16.85968780517578,
      "learning_rate": 2.3224043715846996e-05,
      "loss": 1.4867,
      "step": 980
    },
    {
      "epoch": 5.366120218579235,
      "grad_norm": 13.411946296691895,
      "learning_rate": 2.3169398907103826e-05,
      "loss": 1.0217,
      "step": 982
    },
    {
      "epoch": 5.377049180327869,
      "grad_norm": 12.344143867492676,
      "learning_rate": 2.311475409836066e-05,
      "loss": 1.2147,
      "step": 984
    },
    {
      "epoch": 5.387978142076503,
      "grad_norm": 14.787810325622559,
      "learning_rate": 2.3060109289617486e-05,
      "loss": 1.0548,
      "step": 986
    },
    {
      "epoch": 5.398907103825136,
      "grad_norm": 16.49945640563965,
      "learning_rate": 2.3005464480874316e-05,
      "loss": 1.2432,
      "step": 988
    },
    {
      "epoch": 5.409836065573771,
      "grad_norm": 13.719355583190918,
      "learning_rate": 2.295081967213115e-05,
      "loss": 1.3234,
      "step": 990
    },
    {
      "epoch": 5.420765027322404,
      "grad_norm": 12.477662086486816,
      "learning_rate": 2.289617486338798e-05,
      "loss": 1.1712,
      "step": 992
    },
    {
      "epoch": 5.4316939890710385,
      "grad_norm": 15.428322792053223,
      "learning_rate": 2.284153005464481e-05,
      "loss": 1.4366,
      "step": 994
    },
    {
      "epoch": 5.442622950819672,
      "grad_norm": 15.431221008300781,
      "learning_rate": 2.278688524590164e-05,
      "loss": 1.3974,
      "step": 996
    },
    {
      "epoch": 5.453551912568306,
      "grad_norm": 14.472874641418457,
      "learning_rate": 2.2732240437158473e-05,
      "loss": 1.2054,
      "step": 998
    },
    {
      "epoch": 5.46448087431694,
      "grad_norm": 16.020938873291016,
      "learning_rate": 2.2677595628415303e-05,
      "loss": 1.3991,
      "step": 1000
    },
    {
      "epoch": 5.475409836065574,
      "grad_norm": 14.98514461517334,
      "learning_rate": 2.262295081967213e-05,
      "loss": 1.3231,
      "step": 1002
    },
    {
      "epoch": 5.4863387978142075,
      "grad_norm": 16.271547317504883,
      "learning_rate": 2.2568306010928963e-05,
      "loss": 1.3057,
      "step": 1004
    },
    {
      "epoch": 5.497267759562842,
      "grad_norm": 14.700786590576172,
      "learning_rate": 2.2513661202185793e-05,
      "loss": 1.469,
      "step": 1006
    },
    {
      "epoch": 5.508196721311475,
      "grad_norm": 14.739256858825684,
      "learning_rate": 2.2459016393442626e-05,
      "loss": 1.476,
      "step": 1008
    },
    {
      "epoch": 5.51912568306011,
      "grad_norm": 15.459731101989746,
      "learning_rate": 2.2404371584699453e-05,
      "loss": 1.2183,
      "step": 1010
    },
    {
      "epoch": 5.530054644808743,
      "grad_norm": 14.714408874511719,
      "learning_rate": 2.2349726775956283e-05,
      "loss": 1.3463,
      "step": 1012
    },
    {
      "epoch": 5.540983606557377,
      "grad_norm": 16.979820251464844,
      "learning_rate": 2.2295081967213117e-05,
      "loss": 1.3533,
      "step": 1014
    },
    {
      "epoch": 5.551912568306011,
      "grad_norm": 17.030988693237305,
      "learning_rate": 2.2240437158469947e-05,
      "loss": 1.5052,
      "step": 1016
    },
    {
      "epoch": 5.562841530054644,
      "grad_norm": 14.047904968261719,
      "learning_rate": 2.2185792349726777e-05,
      "loss": 1.2119,
      "step": 1018
    },
    {
      "epoch": 5.573770491803279,
      "grad_norm": 14.914223670959473,
      "learning_rate": 2.2131147540983607e-05,
      "loss": 1.5021,
      "step": 1020
    },
    {
      "epoch": 5.584699453551913,
      "grad_norm": 15.60985279083252,
      "learning_rate": 2.207650273224044e-05,
      "loss": 1.3846,
      "step": 1022
    },
    {
      "epoch": 5.595628415300546,
      "grad_norm": 15.384783744812012,
      "learning_rate": 2.202185792349727e-05,
      "loss": 1.5165,
      "step": 1024
    },
    {
      "epoch": 5.60655737704918,
      "grad_norm": 15.01468276977539,
      "learning_rate": 2.1967213114754097e-05,
      "loss": 1.4589,
      "step": 1026
    },
    {
      "epoch": 5.617486338797814,
      "grad_norm": 15.557282447814941,
      "learning_rate": 2.191256830601093e-05,
      "loss": 1.3877,
      "step": 1028
    },
    {
      "epoch": 5.628415300546449,
      "grad_norm": 17.091554641723633,
      "learning_rate": 2.185792349726776e-05,
      "loss": 1.2472,
      "step": 1030
    },
    {
      "epoch": 5.639344262295082,
      "grad_norm": 14.992789268493652,
      "learning_rate": 2.1803278688524594e-05,
      "loss": 1.3861,
      "step": 1032
    },
    {
      "epoch": 5.6502732240437155,
      "grad_norm": 14.628182411193848,
      "learning_rate": 2.174863387978142e-05,
      "loss": 1.282,
      "step": 1034
    },
    {
      "epoch": 5.66120218579235,
      "grad_norm": 13.803175926208496,
      "learning_rate": 2.1693989071038254e-05,
      "loss": 1.3317,
      "step": 1036
    },
    {
      "epoch": 5.672131147540983,
      "grad_norm": 14.872611999511719,
      "learning_rate": 2.1639344262295084e-05,
      "loss": 1.6463,
      "step": 1038
    },
    {
      "epoch": 5.683060109289618,
      "grad_norm": 15.018031120300293,
      "learning_rate": 2.1584699453551914e-05,
      "loss": 1.6068,
      "step": 1040
    },
    {
      "epoch": 5.693989071038251,
      "grad_norm": 16.10840606689453,
      "learning_rate": 2.1530054644808744e-05,
      "loss": 1.1421,
      "step": 1042
    },
    {
      "epoch": 5.704918032786885,
      "grad_norm": 11.858895301818848,
      "learning_rate": 2.1475409836065574e-05,
      "loss": 1.2059,
      "step": 1044
    },
    {
      "epoch": 5.715846994535519,
      "grad_norm": 14.640716552734375,
      "learning_rate": 2.1420765027322407e-05,
      "loss": 1.4187,
      "step": 1046
    },
    {
      "epoch": 5.726775956284153,
      "grad_norm": 12.933557510375977,
      "learning_rate": 2.1366120218579237e-05,
      "loss": 1.1017,
      "step": 1048
    },
    {
      "epoch": 5.737704918032787,
      "grad_norm": 15.402007102966309,
      "learning_rate": 2.1311475409836064e-05,
      "loss": 1.4532,
      "step": 1050
    },
    {
      "epoch": 5.748633879781421,
      "grad_norm": 14.780879020690918,
      "learning_rate": 2.1256830601092897e-05,
      "loss": 1.0936,
      "step": 1052
    },
    {
      "epoch": 5.759562841530054,
      "grad_norm": 16.224056243896484,
      "learning_rate": 2.1202185792349727e-05,
      "loss": 1.7072,
      "step": 1054
    },
    {
      "epoch": 5.770491803278689,
      "grad_norm": 15.168274879455566,
      "learning_rate": 2.114754098360656e-05,
      "loss": 1.3819,
      "step": 1056
    },
    {
      "epoch": 5.781420765027322,
      "grad_norm": 13.999032974243164,
      "learning_rate": 2.1092896174863387e-05,
      "loss": 1.1355,
      "step": 1058
    },
    {
      "epoch": 5.7923497267759565,
      "grad_norm": 15.675785064697266,
      "learning_rate": 2.103825136612022e-05,
      "loss": 1.2893,
      "step": 1060
    },
    {
      "epoch": 5.80327868852459,
      "grad_norm": 13.583938598632812,
      "learning_rate": 2.098360655737705e-05,
      "loss": 1.2789,
      "step": 1062
    },
    {
      "epoch": 5.814207650273224,
      "grad_norm": 14.437455177307129,
      "learning_rate": 2.092896174863388e-05,
      "loss": 1.4762,
      "step": 1064
    },
    {
      "epoch": 5.825136612021858,
      "grad_norm": 16.1733455657959,
      "learning_rate": 2.087431693989071e-05,
      "loss": 1.5809,
      "step": 1066
    },
    {
      "epoch": 5.836065573770492,
      "grad_norm": 16.269561767578125,
      "learning_rate": 2.081967213114754e-05,
      "loss": 1.4148,
      "step": 1068
    },
    {
      "epoch": 5.8469945355191255,
      "grad_norm": 16.756423950195312,
      "learning_rate": 2.0765027322404374e-05,
      "loss": 1.4465,
      "step": 1070
    },
    {
      "epoch": 5.85792349726776,
      "grad_norm": 17.905317306518555,
      "learning_rate": 2.0710382513661204e-05,
      "loss": 1.3833,
      "step": 1072
    },
    {
      "epoch": 5.868852459016393,
      "grad_norm": 16.700031280517578,
      "learning_rate": 2.0655737704918034e-05,
      "loss": 1.2358,
      "step": 1074
    },
    {
      "epoch": 5.879781420765028,
      "grad_norm": 12.15317153930664,
      "learning_rate": 2.0601092896174865e-05,
      "loss": 0.9237,
      "step": 1076
    },
    {
      "epoch": 5.890710382513661,
      "grad_norm": 15.070958137512207,
      "learning_rate": 2.0546448087431695e-05,
      "loss": 1.382,
      "step": 1078
    },
    {
      "epoch": 5.901639344262295,
      "grad_norm": 15.716472625732422,
      "learning_rate": 2.0491803278688525e-05,
      "loss": 1.4701,
      "step": 1080
    },
    {
      "epoch": 5.912568306010929,
      "grad_norm": 14.433506965637207,
      "learning_rate": 2.0437158469945355e-05,
      "loss": 1.3657,
      "step": 1082
    },
    {
      "epoch": 5.923497267759563,
      "grad_norm": 12.791940689086914,
      "learning_rate": 2.0382513661202188e-05,
      "loss": 1.2766,
      "step": 1084
    },
    {
      "epoch": 5.934426229508197,
      "grad_norm": 15.001571655273438,
      "learning_rate": 2.0327868852459018e-05,
      "loss": 1.4494,
      "step": 1086
    },
    {
      "epoch": 5.945355191256831,
      "grad_norm": 11.866710662841797,
      "learning_rate": 2.0273224043715848e-05,
      "loss": 1.1867,
      "step": 1088
    },
    {
      "epoch": 5.956284153005464,
      "grad_norm": 14.228476524353027,
      "learning_rate": 2.0218579234972678e-05,
      "loss": 1.3586,
      "step": 1090
    },
    {
      "epoch": 5.967213114754099,
      "grad_norm": 14.37263298034668,
      "learning_rate": 2.0163934426229508e-05,
      "loss": 1.437,
      "step": 1092
    },
    {
      "epoch": 5.978142076502732,
      "grad_norm": 13.541136741638184,
      "learning_rate": 2.010928961748634e-05,
      "loss": 1.0053,
      "step": 1094
    },
    {
      "epoch": 5.989071038251366,
      "grad_norm": 15.308109283447266,
      "learning_rate": 2.0054644808743168e-05,
      "loss": 1.2657,
      "step": 1096
    },
    {
      "epoch": 6.0,
      "grad_norm": 15.529770851135254,
      "learning_rate": 2e-05,
      "loss": 1.5046,
      "step": 1098
    },
    {
      "epoch": 6.0109289617486334,
      "grad_norm": 14.047018051147461,
      "learning_rate": 1.994535519125683e-05,
      "loss": 1.2936,
      "step": 1100
    },
    {
      "epoch": 6.021857923497268,
      "grad_norm": 15.01826000213623,
      "learning_rate": 1.9890710382513662e-05,
      "loss": 0.9724,
      "step": 1102
    },
    {
      "epoch": 6.032786885245901,
      "grad_norm": 14.499000549316406,
      "learning_rate": 1.9836065573770492e-05,
      "loss": 1.2934,
      "step": 1104
    },
    {
      "epoch": 6.043715846994536,
      "grad_norm": 13.887956619262695,
      "learning_rate": 1.9781420765027322e-05,
      "loss": 1.0594,
      "step": 1106
    },
    {
      "epoch": 6.054644808743169,
      "grad_norm": 12.538095474243164,
      "learning_rate": 1.9726775956284155e-05,
      "loss": 1.1156,
      "step": 1108
    },
    {
      "epoch": 6.065573770491803,
      "grad_norm": 15.531397819519043,
      "learning_rate": 1.9672131147540985e-05,
      "loss": 1.0991,
      "step": 1110
    },
    {
      "epoch": 6.076502732240437,
      "grad_norm": 15.724651336669922,
      "learning_rate": 1.9617486338797815e-05,
      "loss": 1.3906,
      "step": 1112
    },
    {
      "epoch": 6.087431693989071,
      "grad_norm": 13.612502098083496,
      "learning_rate": 1.9562841530054645e-05,
      "loss": 1.1295,
      "step": 1114
    },
    {
      "epoch": 6.098360655737705,
      "grad_norm": 14.435009002685547,
      "learning_rate": 1.9508196721311475e-05,
      "loss": 0.9723,
      "step": 1116
    },
    {
      "epoch": 6.109289617486339,
      "grad_norm": 14.616153717041016,
      "learning_rate": 1.945355191256831e-05,
      "loss": 1.122,
      "step": 1118
    },
    {
      "epoch": 6.120218579234972,
      "grad_norm": 13.651719093322754,
      "learning_rate": 1.9398907103825135e-05,
      "loss": 1.2491,
      "step": 1120
    },
    {
      "epoch": 6.131147540983607,
      "grad_norm": 12.17211627960205,
      "learning_rate": 1.934426229508197e-05,
      "loss": 1.1986,
      "step": 1122
    },
    {
      "epoch": 6.14207650273224,
      "grad_norm": 15.391507148742676,
      "learning_rate": 1.92896174863388e-05,
      "loss": 1.0256,
      "step": 1124
    },
    {
      "epoch": 6.1530054644808745,
      "grad_norm": 14.560518264770508,
      "learning_rate": 1.9234972677595632e-05,
      "loss": 1.276,
      "step": 1126
    },
    {
      "epoch": 6.163934426229508,
      "grad_norm": 15.56911563873291,
      "learning_rate": 1.918032786885246e-05,
      "loss": 1.1365,
      "step": 1128
    },
    {
      "epoch": 6.174863387978142,
      "grad_norm": 16.854623794555664,
      "learning_rate": 1.912568306010929e-05,
      "loss": 1.252,
      "step": 1130
    },
    {
      "epoch": 6.185792349726776,
      "grad_norm": 19.088191986083984,
      "learning_rate": 1.9071038251366122e-05,
      "loss": 1.3904,
      "step": 1132
    },
    {
      "epoch": 6.19672131147541,
      "grad_norm": 11.316378593444824,
      "learning_rate": 1.9016393442622952e-05,
      "loss": 0.8994,
      "step": 1134
    },
    {
      "epoch": 6.2076502732240435,
      "grad_norm": 14.250025749206543,
      "learning_rate": 1.8961748633879782e-05,
      "loss": 1.1562,
      "step": 1136
    },
    {
      "epoch": 6.218579234972678,
      "grad_norm": 13.238204956054688,
      "learning_rate": 1.8907103825136612e-05,
      "loss": 0.9222,
      "step": 1138
    },
    {
      "epoch": 6.229508196721311,
      "grad_norm": 11.721009254455566,
      "learning_rate": 1.8852459016393442e-05,
      "loss": 0.9134,
      "step": 1140
    },
    {
      "epoch": 6.240437158469946,
      "grad_norm": 17.11862564086914,
      "learning_rate": 1.8797814207650276e-05,
      "loss": 1.462,
      "step": 1142
    },
    {
      "epoch": 6.251366120218579,
      "grad_norm": 14.600622177124023,
      "learning_rate": 1.8743169398907103e-05,
      "loss": 1.3292,
      "step": 1144
    },
    {
      "epoch": 6.262295081967213,
      "grad_norm": 11.617315292358398,
      "learning_rate": 1.8688524590163936e-05,
      "loss": 0.9578,
      "step": 1146
    },
    {
      "epoch": 6.273224043715847,
      "grad_norm": 12.171364784240723,
      "learning_rate": 1.8633879781420766e-05,
      "loss": 0.9837,
      "step": 1148
    },
    {
      "epoch": 6.284153005464481,
      "grad_norm": 14.667792320251465,
      "learning_rate": 1.85792349726776e-05,
      "loss": 1.2078,
      "step": 1150
    },
    {
      "epoch": 6.295081967213115,
      "grad_norm": 15.111309051513672,
      "learning_rate": 1.8524590163934426e-05,
      "loss": 1.2228,
      "step": 1152
    },
    {
      "epoch": 6.306010928961749,
      "grad_norm": 14.36209487915039,
      "learning_rate": 1.8469945355191256e-05,
      "loss": 1.0749,
      "step": 1154
    },
    {
      "epoch": 6.316939890710382,
      "grad_norm": 15.14391040802002,
      "learning_rate": 1.841530054644809e-05,
      "loss": 1.3106,
      "step": 1156
    },
    {
      "epoch": 6.327868852459017,
      "grad_norm": 14.275545120239258,
      "learning_rate": 1.836065573770492e-05,
      "loss": 1.0299,
      "step": 1158
    },
    {
      "epoch": 6.33879781420765,
      "grad_norm": 15.720427513122559,
      "learning_rate": 1.830601092896175e-05,
      "loss": 1.4537,
      "step": 1160
    },
    {
      "epoch": 6.3497267759562845,
      "grad_norm": 11.165779113769531,
      "learning_rate": 1.825136612021858e-05,
      "loss": 0.7766,
      "step": 1162
    },
    {
      "epoch": 6.360655737704918,
      "grad_norm": 12.595122337341309,
      "learning_rate": 1.8196721311475413e-05,
      "loss": 0.9148,
      "step": 1164
    },
    {
      "epoch": 6.371584699453552,
      "grad_norm": 16.7065372467041,
      "learning_rate": 1.8142076502732243e-05,
      "loss": 1.245,
      "step": 1166
    },
    {
      "epoch": 6.382513661202186,
      "grad_norm": 14.7833890914917,
      "learning_rate": 1.808743169398907e-05,
      "loss": 1.0486,
      "step": 1168
    },
    {
      "epoch": 6.39344262295082,
      "grad_norm": 12.687151908874512,
      "learning_rate": 1.8032786885245903e-05,
      "loss": 1.1738,
      "step": 1170
    },
    {
      "epoch": 6.404371584699454,
      "grad_norm": 16.648942947387695,
      "learning_rate": 1.7978142076502733e-05,
      "loss": 1.0571,
      "step": 1172
    },
    {
      "epoch": 6.415300546448087,
      "grad_norm": 11.81535530090332,
      "learning_rate": 1.7923497267759563e-05,
      "loss": 0.8555,
      "step": 1174
    },
    {
      "epoch": 6.426229508196721,
      "grad_norm": 13.89103889465332,
      "learning_rate": 1.7868852459016393e-05,
      "loss": 1.19,
      "step": 1176
    },
    {
      "epoch": 6.437158469945355,
      "grad_norm": 12.990193367004395,
      "learning_rate": 1.7814207650273223e-05,
      "loss": 1.3179,
      "step": 1178
    },
    {
      "epoch": 6.448087431693989,
      "grad_norm": 15.819822311401367,
      "learning_rate": 1.7759562841530057e-05,
      "loss": 1.2521,
      "step": 1180
    },
    {
      "epoch": 6.459016393442623,
      "grad_norm": 12.067497253417969,
      "learning_rate": 1.7704918032786887e-05,
      "loss": 1.0912,
      "step": 1182
    },
    {
      "epoch": 6.469945355191257,
      "grad_norm": 15.950645446777344,
      "learning_rate": 1.7650273224043717e-05,
      "loss": 1.1597,
      "step": 1184
    },
    {
      "epoch": 6.48087431693989,
      "grad_norm": 14.275954246520996,
      "learning_rate": 1.7595628415300547e-05,
      "loss": 1.1897,
      "step": 1186
    },
    {
      "epoch": 6.491803278688525,
      "grad_norm": 17.183807373046875,
      "learning_rate": 1.754098360655738e-05,
      "loss": 1.4248,
      "step": 1188
    },
    {
      "epoch": 6.502732240437158,
      "grad_norm": 14.525217056274414,
      "learning_rate": 1.7486338797814207e-05,
      "loss": 1.203,
      "step": 1190
    },
    {
      "epoch": 6.5136612021857925,
      "grad_norm": 11.561469078063965,
      "learning_rate": 1.7431693989071037e-05,
      "loss": 1.0863,
      "step": 1192
    },
    {
      "epoch": 6.524590163934426,
      "grad_norm": 14.015625,
      "learning_rate": 1.737704918032787e-05,
      "loss": 1.1421,
      "step": 1194
    },
    {
      "epoch": 6.53551912568306,
      "grad_norm": 14.02107048034668,
      "learning_rate": 1.73224043715847e-05,
      "loss": 1.3072,
      "step": 1196
    },
    {
      "epoch": 6.546448087431694,
      "grad_norm": 13.999459266662598,
      "learning_rate": 1.726775956284153e-05,
      "loss": 1.0469,
      "step": 1198
    },
    {
      "epoch": 6.557377049180328,
      "grad_norm": 13.860760688781738,
      "learning_rate": 1.721311475409836e-05,
      "loss": 1.092,
      "step": 1200
    },
    {
      "epoch": 6.5683060109289615,
      "grad_norm": 12.792620658874512,
      "learning_rate": 1.7158469945355194e-05,
      "loss": 1.0315,
      "step": 1202
    },
    {
      "epoch": 6.579234972677596,
      "grad_norm": 11.259674072265625,
      "learning_rate": 1.7103825136612024e-05,
      "loss": 1.1128,
      "step": 1204
    },
    {
      "epoch": 6.590163934426229,
      "grad_norm": 11.801107406616211,
      "learning_rate": 1.7049180327868854e-05,
      "loss": 1.0691,
      "step": 1206
    },
    {
      "epoch": 6.601092896174864,
      "grad_norm": 14.811394691467285,
      "learning_rate": 1.6994535519125684e-05,
      "loss": 1.3308,
      "step": 1208
    },
    {
      "epoch": 6.612021857923497,
      "grad_norm": 12.73621940612793,
      "learning_rate": 1.6939890710382514e-05,
      "loss": 1.2778,
      "step": 1210
    },
    {
      "epoch": 6.622950819672131,
      "grad_norm": 15.03689193725586,
      "learning_rate": 1.6885245901639347e-05,
      "loss": 0.9477,
      "step": 1212
    },
    {
      "epoch": 6.633879781420765,
      "grad_norm": 12.411208152770996,
      "learning_rate": 1.6830601092896174e-05,
      "loss": 1.1097,
      "step": 1214
    },
    {
      "epoch": 6.644808743169399,
      "grad_norm": 15.679587364196777,
      "learning_rate": 1.6775956284153004e-05,
      "loss": 1.2344,
      "step": 1216
    },
    {
      "epoch": 6.655737704918033,
      "grad_norm": 10.766380310058594,
      "learning_rate": 1.6721311475409837e-05,
      "loss": 0.9803,
      "step": 1218
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 15.868833541870117,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.947,
      "step": 1220
    },
    {
      "epoch": 6.6775956284153,
      "grad_norm": 16.866680145263672,
      "learning_rate": 1.6612021857923497e-05,
      "loss": 1.3823,
      "step": 1222
    },
    {
      "epoch": 6.688524590163935,
      "grad_norm": 12.868598937988281,
      "learning_rate": 1.6557377049180328e-05,
      "loss": 1.1904,
      "step": 1224
    },
    {
      "epoch": 6.699453551912568,
      "grad_norm": 15.859928131103516,
      "learning_rate": 1.650273224043716e-05,
      "loss": 1.358,
      "step": 1226
    },
    {
      "epoch": 6.7103825136612025,
      "grad_norm": 14.6238374710083,
      "learning_rate": 1.644808743169399e-05,
      "loss": 1.1825,
      "step": 1228
    },
    {
      "epoch": 6.721311475409836,
      "grad_norm": 14.937554359436035,
      "learning_rate": 1.6393442622950818e-05,
      "loss": 1.1817,
      "step": 1230
    },
    {
      "epoch": 6.73224043715847,
      "grad_norm": 14.307670593261719,
      "learning_rate": 1.633879781420765e-05,
      "loss": 1.038,
      "step": 1232
    },
    {
      "epoch": 6.743169398907104,
      "grad_norm": 13.39194393157959,
      "learning_rate": 1.628415300546448e-05,
      "loss": 1.1028,
      "step": 1234
    },
    {
      "epoch": 6.754098360655737,
      "grad_norm": 13.303352355957031,
      "learning_rate": 1.6229508196721314e-05,
      "loss": 1.2629,
      "step": 1236
    },
    {
      "epoch": 6.7650273224043715,
      "grad_norm": 15.460023880004883,
      "learning_rate": 1.617486338797814e-05,
      "loss": 1.328,
      "step": 1238
    },
    {
      "epoch": 6.775956284153006,
      "grad_norm": 15.61375904083252,
      "learning_rate": 1.6120218579234975e-05,
      "loss": 1.3089,
      "step": 1240
    },
    {
      "epoch": 6.786885245901639,
      "grad_norm": 16.1142635345459,
      "learning_rate": 1.6065573770491805e-05,
      "loss": 1.058,
      "step": 1242
    },
    {
      "epoch": 6.797814207650273,
      "grad_norm": 12.625345230102539,
      "learning_rate": 1.6010928961748635e-05,
      "loss": 0.939,
      "step": 1244
    },
    {
      "epoch": 6.808743169398907,
      "grad_norm": 16.11082649230957,
      "learning_rate": 1.5956284153005465e-05,
      "loss": 1.2101,
      "step": 1246
    },
    {
      "epoch": 6.8196721311475414,
      "grad_norm": 14.169987678527832,
      "learning_rate": 1.5901639344262295e-05,
      "loss": 1.2104,
      "step": 1248
    },
    {
      "epoch": 6.830601092896175,
      "grad_norm": 15.169610977172852,
      "learning_rate": 1.5846994535519128e-05,
      "loss": 1.2157,
      "step": 1250
    },
    {
      "epoch": 6.841530054644808,
      "grad_norm": 14.41938304901123,
      "learning_rate": 1.5792349726775958e-05,
      "loss": 1.2398,
      "step": 1252
    },
    {
      "epoch": 6.852459016393443,
      "grad_norm": 15.150559425354004,
      "learning_rate": 1.5737704918032788e-05,
      "loss": 1.2991,
      "step": 1254
    },
    {
      "epoch": 6.863387978142076,
      "grad_norm": 16.106435775756836,
      "learning_rate": 1.5683060109289618e-05,
      "loss": 1.2107,
      "step": 1256
    },
    {
      "epoch": 6.8743169398907105,
      "grad_norm": 16.39731788635254,
      "learning_rate": 1.5628415300546448e-05,
      "loss": 1.2504,
      "step": 1258
    },
    {
      "epoch": 6.885245901639344,
      "grad_norm": 15.844545364379883,
      "learning_rate": 1.557377049180328e-05,
      "loss": 1.1323,
      "step": 1260
    },
    {
      "epoch": 6.896174863387978,
      "grad_norm": 15.44594955444336,
      "learning_rate": 1.5519125683060108e-05,
      "loss": 1.1433,
      "step": 1262
    },
    {
      "epoch": 6.907103825136612,
      "grad_norm": 12.777754783630371,
      "learning_rate": 1.5464480874316942e-05,
      "loss": 1.2208,
      "step": 1264
    },
    {
      "epoch": 6.918032786885246,
      "grad_norm": 12.278426170349121,
      "learning_rate": 1.5409836065573772e-05,
      "loss": 0.9702,
      "step": 1266
    },
    {
      "epoch": 6.9289617486338795,
      "grad_norm": 16.034536361694336,
      "learning_rate": 1.5355191256830602e-05,
      "loss": 1.1582,
      "step": 1268
    },
    {
      "epoch": 6.939890710382514,
      "grad_norm": 11.751357078552246,
      "learning_rate": 1.5300546448087432e-05,
      "loss": 0.9888,
      "step": 1270
    },
    {
      "epoch": 6.950819672131147,
      "grad_norm": 12.941003799438477,
      "learning_rate": 1.5245901639344262e-05,
      "loss": 1.0329,
      "step": 1272
    },
    {
      "epoch": 6.961748633879782,
      "grad_norm": 14.738349914550781,
      "learning_rate": 1.5191256830601094e-05,
      "loss": 1.1487,
      "step": 1274
    },
    {
      "epoch": 6.972677595628415,
      "grad_norm": 15.151143074035645,
      "learning_rate": 1.5136612021857924e-05,
      "loss": 1.1518,
      "step": 1276
    },
    {
      "epoch": 6.983606557377049,
      "grad_norm": 12.609070777893066,
      "learning_rate": 1.5081967213114755e-05,
      "loss": 1.0235,
      "step": 1278
    },
    {
      "epoch": 6.994535519125683,
      "grad_norm": 15.22632122039795,
      "learning_rate": 1.5027322404371585e-05,
      "loss": 1.3095,
      "step": 1280
    },
    {
      "epoch": 7.005464480874317,
      "grad_norm": 14.891922950744629,
      "learning_rate": 1.4972677595628415e-05,
      "loss": 1.2872,
      "step": 1282
    },
    {
      "epoch": 7.016393442622951,
      "grad_norm": 11.288215637207031,
      "learning_rate": 1.4918032786885247e-05,
      "loss": 1.1346,
      "step": 1284
    },
    {
      "epoch": 7.027322404371585,
      "grad_norm": 10.71212100982666,
      "learning_rate": 1.4863387978142077e-05,
      "loss": 0.9284,
      "step": 1286
    },
    {
      "epoch": 7.038251366120218,
      "grad_norm": 13.356491088867188,
      "learning_rate": 1.4808743169398909e-05,
      "loss": 1.1579,
      "step": 1288
    },
    {
      "epoch": 7.049180327868853,
      "grad_norm": 16.48250961303711,
      "learning_rate": 1.4754098360655739e-05,
      "loss": 1.0004,
      "step": 1290
    },
    {
      "epoch": 7.060109289617486,
      "grad_norm": 13.445688247680664,
      "learning_rate": 1.469945355191257e-05,
      "loss": 1.0467,
      "step": 1292
    },
    {
      "epoch": 7.0710382513661205,
      "grad_norm": 14.894604682922363,
      "learning_rate": 1.4644808743169399e-05,
      "loss": 1.1628,
      "step": 1294
    },
    {
      "epoch": 7.081967213114754,
      "grad_norm": 15.043351173400879,
      "learning_rate": 1.4590163934426229e-05,
      "loss": 1.1299,
      "step": 1296
    },
    {
      "epoch": 7.092896174863388,
      "grad_norm": 12.531146049499512,
      "learning_rate": 1.453551912568306e-05,
      "loss": 0.7093,
      "step": 1298
    },
    {
      "epoch": 7.103825136612022,
      "grad_norm": 12.823294639587402,
      "learning_rate": 1.448087431693989e-05,
      "loss": 1.0837,
      "step": 1300
    },
    {
      "epoch": 7.114754098360656,
      "grad_norm": 9.998366355895996,
      "learning_rate": 1.4426229508196722e-05,
      "loss": 0.8289,
      "step": 1302
    },
    {
      "epoch": 7.1256830601092895,
      "grad_norm": 12.59792709350586,
      "learning_rate": 1.4371584699453553e-05,
      "loss": 1.0387,
      "step": 1304
    },
    {
      "epoch": 7.136612021857924,
      "grad_norm": 13.378044128417969,
      "learning_rate": 1.4316939890710383e-05,
      "loss": 1.0026,
      "step": 1306
    },
    {
      "epoch": 7.147540983606557,
      "grad_norm": 14.288396835327148,
      "learning_rate": 1.4262295081967214e-05,
      "loss": 1.187,
      "step": 1308
    },
    {
      "epoch": 7.158469945355192,
      "grad_norm": 12.903855323791504,
      "learning_rate": 1.4207650273224044e-05,
      "loss": 1.1438,
      "step": 1310
    },
    {
      "epoch": 7.169398907103825,
      "grad_norm": 12.570511817932129,
      "learning_rate": 1.4153005464480876e-05,
      "loss": 0.9944,
      "step": 1312
    },
    {
      "epoch": 7.180327868852459,
      "grad_norm": 14.994438171386719,
      "learning_rate": 1.4098360655737704e-05,
      "loss": 1.06,
      "step": 1314
    },
    {
      "epoch": 7.191256830601093,
      "grad_norm": 16.263134002685547,
      "learning_rate": 1.4043715846994538e-05,
      "loss": 1.1908,
      "step": 1316
    },
    {
      "epoch": 7.202185792349727,
      "grad_norm": 14.162620544433594,
      "learning_rate": 1.3989071038251366e-05,
      "loss": 1.3026,
      "step": 1318
    },
    {
      "epoch": 7.213114754098361,
      "grad_norm": 16.154264450073242,
      "learning_rate": 1.3934426229508196e-05,
      "loss": 1.1415,
      "step": 1320
    },
    {
      "epoch": 7.224043715846994,
      "grad_norm": 14.675524711608887,
      "learning_rate": 1.3879781420765028e-05,
      "loss": 1.2705,
      "step": 1322
    },
    {
      "epoch": 7.2349726775956285,
      "grad_norm": 10.85481071472168,
      "learning_rate": 1.3825136612021858e-05,
      "loss": 0.7029,
      "step": 1324
    },
    {
      "epoch": 7.245901639344262,
      "grad_norm": 13.175727844238281,
      "learning_rate": 1.377049180327869e-05,
      "loss": 1.0755,
      "step": 1326
    },
    {
      "epoch": 7.256830601092896,
      "grad_norm": 11.631747245788574,
      "learning_rate": 1.371584699453552e-05,
      "loss": 0.6496,
      "step": 1328
    },
    {
      "epoch": 7.26775956284153,
      "grad_norm": 13.4945068359375,
      "learning_rate": 1.3661202185792351e-05,
      "loss": 1.1378,
      "step": 1330
    },
    {
      "epoch": 7.278688524590164,
      "grad_norm": 11.413687705993652,
      "learning_rate": 1.3606557377049181e-05,
      "loss": 0.8328,
      "step": 1332
    },
    {
      "epoch": 7.2896174863387975,
      "grad_norm": 13.671053886413574,
      "learning_rate": 1.355191256830601e-05,
      "loss": 1.0486,
      "step": 1334
    },
    {
      "epoch": 7.300546448087432,
      "grad_norm": 13.692475318908691,
      "learning_rate": 1.3497267759562843e-05,
      "loss": 1.1319,
      "step": 1336
    },
    {
      "epoch": 7.311475409836065,
      "grad_norm": 12.638508796691895,
      "learning_rate": 1.3442622950819672e-05,
      "loss": 0.824,
      "step": 1338
    },
    {
      "epoch": 7.3224043715847,
      "grad_norm": 14.709939956665039,
      "learning_rate": 1.3387978142076505e-05,
      "loss": 1.2288,
      "step": 1340
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 12.545089721679688,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.772,
      "step": 1342
    },
    {
      "epoch": 7.344262295081967,
      "grad_norm": 16.241437911987305,
      "learning_rate": 1.3278688524590163e-05,
      "loss": 0.8846,
      "step": 1344
    },
    {
      "epoch": 7.355191256830601,
      "grad_norm": 13.774543762207031,
      "learning_rate": 1.3224043715846995e-05,
      "loss": 0.9688,
      "step": 1346
    },
    {
      "epoch": 7.366120218579235,
      "grad_norm": 13.84518814086914,
      "learning_rate": 1.3169398907103825e-05,
      "loss": 0.9941,
      "step": 1348
    },
    {
      "epoch": 7.377049180327869,
      "grad_norm": 13.82197093963623,
      "learning_rate": 1.3114754098360657e-05,
      "loss": 1.1356,
      "step": 1350
    },
    {
      "epoch": 7.387978142076503,
      "grad_norm": 13.542481422424316,
      "learning_rate": 1.3060109289617487e-05,
      "loss": 1.1055,
      "step": 1352
    },
    {
      "epoch": 7.398907103825136,
      "grad_norm": 15.19571590423584,
      "learning_rate": 1.3005464480874319e-05,
      "loss": 1.0477,
      "step": 1354
    },
    {
      "epoch": 7.409836065573771,
      "grad_norm": 10.075241088867188,
      "learning_rate": 1.2950819672131149e-05,
      "loss": 0.7277,
      "step": 1356
    },
    {
      "epoch": 7.420765027322404,
      "grad_norm": 12.9566068649292,
      "learning_rate": 1.2896174863387977e-05,
      "loss": 0.9401,
      "step": 1358
    },
    {
      "epoch": 7.4316939890710385,
      "grad_norm": 14.022618293762207,
      "learning_rate": 1.284153005464481e-05,
      "loss": 1.1995,
      "step": 1360
    },
    {
      "epoch": 7.442622950819672,
      "grad_norm": 11.201584815979004,
      "learning_rate": 1.2786885245901639e-05,
      "loss": 0.8324,
      "step": 1362
    },
    {
      "epoch": 7.453551912568306,
      "grad_norm": 11.206255912780762,
      "learning_rate": 1.2732240437158472e-05,
      "loss": 0.8333,
      "step": 1364
    },
    {
      "epoch": 7.46448087431694,
      "grad_norm": 14.212936401367188,
      "learning_rate": 1.26775956284153e-05,
      "loss": 0.9254,
      "step": 1366
    },
    {
      "epoch": 7.475409836065574,
      "grad_norm": 13.234752655029297,
      "learning_rate": 1.2622950819672132e-05,
      "loss": 0.9575,
      "step": 1368
    },
    {
      "epoch": 7.4863387978142075,
      "grad_norm": 11.177515029907227,
      "learning_rate": 1.2568306010928962e-05,
      "loss": 1.0413,
      "step": 1370
    },
    {
      "epoch": 7.497267759562842,
      "grad_norm": 12.85063362121582,
      "learning_rate": 1.2513661202185792e-05,
      "loss": 1.0225,
      "step": 1372
    },
    {
      "epoch": 7.508196721311475,
      "grad_norm": 14.085360527038574,
      "learning_rate": 1.2459016393442624e-05,
      "loss": 1.2438,
      "step": 1374
    },
    {
      "epoch": 7.51912568306011,
      "grad_norm": 13.184494018554688,
      "learning_rate": 1.2404371584699454e-05,
      "loss": 0.8618,
      "step": 1376
    },
    {
      "epoch": 7.530054644808743,
      "grad_norm": 14.420397758483887,
      "learning_rate": 1.2349726775956284e-05,
      "loss": 1.1558,
      "step": 1378
    },
    {
      "epoch": 7.540983606557377,
      "grad_norm": 13.062408447265625,
      "learning_rate": 1.2295081967213116e-05,
      "loss": 1.0889,
      "step": 1380
    },
    {
      "epoch": 7.551912568306011,
      "grad_norm": 15.972475051879883,
      "learning_rate": 1.2240437158469946e-05,
      "loss": 1.0208,
      "step": 1382
    },
    {
      "epoch": 7.562841530054644,
      "grad_norm": 11.420150756835938,
      "learning_rate": 1.2185792349726778e-05,
      "loss": 0.9709,
      "step": 1384
    },
    {
      "epoch": 7.573770491803279,
      "grad_norm": 11.440299034118652,
      "learning_rate": 1.2131147540983608e-05,
      "loss": 0.8081,
      "step": 1386
    },
    {
      "epoch": 7.584699453551913,
      "grad_norm": 14.50943660736084,
      "learning_rate": 1.2076502732240438e-05,
      "loss": 0.9558,
      "step": 1388
    },
    {
      "epoch": 7.595628415300546,
      "grad_norm": 15.166133880615234,
      "learning_rate": 1.2021857923497268e-05,
      "loss": 0.9835,
      "step": 1390
    },
    {
      "epoch": 7.60655737704918,
      "grad_norm": 15.787537574768066,
      "learning_rate": 1.19672131147541e-05,
      "loss": 1.2383,
      "step": 1392
    },
    {
      "epoch": 7.617486338797814,
      "grad_norm": 14.698847770690918,
      "learning_rate": 1.191256830601093e-05,
      "loss": 1.1409,
      "step": 1394
    },
    {
      "epoch": 7.628415300546449,
      "grad_norm": 12.21990966796875,
      "learning_rate": 1.1857923497267761e-05,
      "loss": 0.9893,
      "step": 1396
    },
    {
      "epoch": 7.639344262295082,
      "grad_norm": 12.258447647094727,
      "learning_rate": 1.1803278688524591e-05,
      "loss": 0.8855,
      "step": 1398
    },
    {
      "epoch": 7.6502732240437155,
      "grad_norm": 13.656875610351562,
      "learning_rate": 1.1748633879781421e-05,
      "loss": 0.987,
      "step": 1400
    },
    {
      "epoch": 7.66120218579235,
      "grad_norm": 13.684968948364258,
      "learning_rate": 1.1693989071038251e-05,
      "loss": 0.9954,
      "step": 1402
    },
    {
      "epoch": 7.672131147540983,
      "grad_norm": 11.23112678527832,
      "learning_rate": 1.1639344262295083e-05,
      "loss": 0.8677,
      "step": 1404
    },
    {
      "epoch": 7.683060109289618,
      "grad_norm": 13.26708698272705,
      "learning_rate": 1.1584699453551913e-05,
      "loss": 1.0002,
      "step": 1406
    },
    {
      "epoch": 7.693989071038251,
      "grad_norm": 12.187454223632812,
      "learning_rate": 1.1530054644808743e-05,
      "loss": 0.741,
      "step": 1408
    },
    {
      "epoch": 7.704918032786885,
      "grad_norm": 9.421491622924805,
      "learning_rate": 1.1475409836065575e-05,
      "loss": 0.6915,
      "step": 1410
    },
    {
      "epoch": 7.715846994535519,
      "grad_norm": 14.20734691619873,
      "learning_rate": 1.1420765027322405e-05,
      "loss": 1.2417,
      "step": 1412
    },
    {
      "epoch": 7.726775956284153,
      "grad_norm": 13.510947227478027,
      "learning_rate": 1.1366120218579236e-05,
      "loss": 1.0751,
      "step": 1414
    },
    {
      "epoch": 7.737704918032787,
      "grad_norm": 14.343244552612305,
      "learning_rate": 1.1311475409836065e-05,
      "loss": 1.0174,
      "step": 1416
    },
    {
      "epoch": 7.748633879781421,
      "grad_norm": 12.920513153076172,
      "learning_rate": 1.1256830601092897e-05,
      "loss": 0.9959,
      "step": 1418
    },
    {
      "epoch": 7.759562841530054,
      "grad_norm": 14.451539993286133,
      "learning_rate": 1.1202185792349727e-05,
      "loss": 1.0688,
      "step": 1420
    },
    {
      "epoch": 7.770491803278689,
      "grad_norm": 12.147286415100098,
      "learning_rate": 1.1147540983606558e-05,
      "loss": 0.8815,
      "step": 1422
    },
    {
      "epoch": 7.781420765027322,
      "grad_norm": 13.099087715148926,
      "learning_rate": 1.1092896174863388e-05,
      "loss": 0.8949,
      "step": 1424
    },
    {
      "epoch": 7.7923497267759565,
      "grad_norm": 14.865025520324707,
      "learning_rate": 1.103825136612022e-05,
      "loss": 1.1696,
      "step": 1426
    },
    {
      "epoch": 7.80327868852459,
      "grad_norm": 14.37851333618164,
      "learning_rate": 1.0983606557377048e-05,
      "loss": 1.1596,
      "step": 1428
    },
    {
      "epoch": 7.814207650273224,
      "grad_norm": 10.79284954071045,
      "learning_rate": 1.092896174863388e-05,
      "loss": 0.8408,
      "step": 1430
    },
    {
      "epoch": 7.825136612021858,
      "grad_norm": 16.18177032470703,
      "learning_rate": 1.087431693989071e-05,
      "loss": 1.2789,
      "step": 1432
    },
    {
      "epoch": 7.836065573770492,
      "grad_norm": 15.282419204711914,
      "learning_rate": 1.0819672131147542e-05,
      "loss": 1.1598,
      "step": 1434
    },
    {
      "epoch": 7.8469945355191255,
      "grad_norm": 14.786747932434082,
      "learning_rate": 1.0765027322404372e-05,
      "loss": 1.0498,
      "step": 1436
    },
    {
      "epoch": 7.85792349726776,
      "grad_norm": 15.985187530517578,
      "learning_rate": 1.0710382513661204e-05,
      "loss": 1.1444,
      "step": 1438
    },
    {
      "epoch": 7.868852459016393,
      "grad_norm": 12.088785171508789,
      "learning_rate": 1.0655737704918032e-05,
      "loss": 1.0008,
      "step": 1440
    },
    {
      "epoch": 7.879781420765028,
      "grad_norm": 14.990052223205566,
      "learning_rate": 1.0601092896174864e-05,
      "loss": 1.1645,
      "step": 1442
    },
    {
      "epoch": 7.890710382513661,
      "grad_norm": 14.521008491516113,
      "learning_rate": 1.0546448087431694e-05,
      "loss": 1.2237,
      "step": 1444
    },
    {
      "epoch": 7.901639344262295,
      "grad_norm": 12.808450698852539,
      "learning_rate": 1.0491803278688525e-05,
      "loss": 1.1553,
      "step": 1446
    },
    {
      "epoch": 7.912568306010929,
      "grad_norm": 10.955839157104492,
      "learning_rate": 1.0437158469945355e-05,
      "loss": 0.9054,
      "step": 1448
    },
    {
      "epoch": 7.923497267759563,
      "grad_norm": 12.390751838684082,
      "learning_rate": 1.0382513661202187e-05,
      "loss": 1.0178,
      "step": 1450
    },
    {
      "epoch": 7.934426229508197,
      "grad_norm": 14.728737831115723,
      "learning_rate": 1.0327868852459017e-05,
      "loss": 1.1431,
      "step": 1452
    },
    {
      "epoch": 7.945355191256831,
      "grad_norm": 14.458657264709473,
      "learning_rate": 1.0273224043715847e-05,
      "loss": 1.0826,
      "step": 1454
    },
    {
      "epoch": 7.956284153005464,
      "grad_norm": 13.965533256530762,
      "learning_rate": 1.0218579234972677e-05,
      "loss": 1.2021,
      "step": 1456
    },
    {
      "epoch": 7.967213114754099,
      "grad_norm": 14.850200653076172,
      "learning_rate": 1.0163934426229509e-05,
      "loss": 0.9682,
      "step": 1458
    },
    {
      "epoch": 7.978142076502732,
      "grad_norm": 15.042131423950195,
      "learning_rate": 1.0109289617486339e-05,
      "loss": 1.2373,
      "step": 1460
    },
    {
      "epoch": 7.989071038251366,
      "grad_norm": 13.071454048156738,
      "learning_rate": 1.005464480874317e-05,
      "loss": 0.9373,
      "step": 1462
    },
    {
      "epoch": 8.0,
      "grad_norm": 14.56152629852295,
      "learning_rate": 1e-05,
      "loss": 1.2024,
      "step": 1464
    },
    {
      "epoch": 8.010928961748634,
      "grad_norm": 15.316999435424805,
      "learning_rate": 9.945355191256831e-06,
      "loss": 1.0478,
      "step": 1466
    },
    {
      "epoch": 8.021857923497267,
      "grad_norm": 8.378629684448242,
      "learning_rate": 9.890710382513661e-06,
      "loss": 0.6303,
      "step": 1468
    },
    {
      "epoch": 8.032786885245901,
      "grad_norm": 13.204364776611328,
      "learning_rate": 9.836065573770493e-06,
      "loss": 0.7477,
      "step": 1470
    },
    {
      "epoch": 8.043715846994536,
      "grad_norm": 12.028918266296387,
      "learning_rate": 9.781420765027323e-06,
      "loss": 0.9717,
      "step": 1472
    },
    {
      "epoch": 8.05464480874317,
      "grad_norm": 13.076743125915527,
      "learning_rate": 9.726775956284154e-06,
      "loss": 0.8213,
      "step": 1474
    },
    {
      "epoch": 8.065573770491802,
      "grad_norm": 14.078008651733398,
      "learning_rate": 9.672131147540984e-06,
      "loss": 1.0831,
      "step": 1476
    },
    {
      "epoch": 8.076502732240437,
      "grad_norm": 15.094325065612793,
      "learning_rate": 9.617486338797816e-06,
      "loss": 0.9586,
      "step": 1478
    },
    {
      "epoch": 8.087431693989071,
      "grad_norm": 11.207356452941895,
      "learning_rate": 9.562841530054644e-06,
      "loss": 0.6257,
      "step": 1480
    },
    {
      "epoch": 8.098360655737705,
      "grad_norm": 14.613252639770508,
      "learning_rate": 9.508196721311476e-06,
      "loss": 0.8924,
      "step": 1482
    },
    {
      "epoch": 8.109289617486338,
      "grad_norm": 9.246810913085938,
      "learning_rate": 9.453551912568306e-06,
      "loss": 0.7576,
      "step": 1484
    },
    {
      "epoch": 8.120218579234972,
      "grad_norm": 13.609003067016602,
      "learning_rate": 9.398907103825138e-06,
      "loss": 1.0828,
      "step": 1486
    },
    {
      "epoch": 8.131147540983607,
      "grad_norm": 12.703204154968262,
      "learning_rate": 9.344262295081968e-06,
      "loss": 1.0868,
      "step": 1488
    },
    {
      "epoch": 8.142076502732241,
      "grad_norm": 26.616994857788086,
      "learning_rate": 9.2896174863388e-06,
      "loss": 1.1004,
      "step": 1490
    },
    {
      "epoch": 8.153005464480874,
      "grad_norm": 15.319024085998535,
      "learning_rate": 9.234972677595628e-06,
      "loss": 1.1837,
      "step": 1492
    },
    {
      "epoch": 8.163934426229508,
      "grad_norm": 10.55885124206543,
      "learning_rate": 9.18032786885246e-06,
      "loss": 0.6741,
      "step": 1494
    },
    {
      "epoch": 8.174863387978142,
      "grad_norm": 13.423583984375,
      "learning_rate": 9.12568306010929e-06,
      "loss": 1.0173,
      "step": 1496
    },
    {
      "epoch": 8.185792349726777,
      "grad_norm": 10.177030563354492,
      "learning_rate": 9.071038251366122e-06,
      "loss": 0.8072,
      "step": 1498
    },
    {
      "epoch": 8.19672131147541,
      "grad_norm": 11.613557815551758,
      "learning_rate": 9.016393442622952e-06,
      "loss": 0.9471,
      "step": 1500
    },
    {
      "epoch": 8.207650273224044,
      "grad_norm": 13.083951950073242,
      "learning_rate": 8.961748633879782e-06,
      "loss": 0.9525,
      "step": 1502
    },
    {
      "epoch": 8.218579234972678,
      "grad_norm": 15.7395658493042,
      "learning_rate": 8.907103825136612e-06,
      "loss": 1.2308,
      "step": 1504
    },
    {
      "epoch": 8.229508196721312,
      "grad_norm": 15.160648345947266,
      "learning_rate": 8.852459016393443e-06,
      "loss": 0.8548,
      "step": 1506
    },
    {
      "epoch": 8.240437158469945,
      "grad_norm": 15.193538665771484,
      "learning_rate": 8.797814207650273e-06,
      "loss": 0.9313,
      "step": 1508
    },
    {
      "epoch": 8.251366120218579,
      "grad_norm": 20.272884368896484,
      "learning_rate": 8.743169398907103e-06,
      "loss": 1.1382,
      "step": 1510
    },
    {
      "epoch": 8.262295081967213,
      "grad_norm": 13.465476036071777,
      "learning_rate": 8.688524590163935e-06,
      "loss": 0.9759,
      "step": 1512
    },
    {
      "epoch": 8.273224043715848,
      "grad_norm": 8.761789321899414,
      "learning_rate": 8.633879781420765e-06,
      "loss": 0.7341,
      "step": 1514
    },
    {
      "epoch": 8.28415300546448,
      "grad_norm": 11.23634147644043,
      "learning_rate": 8.579234972677597e-06,
      "loss": 0.8769,
      "step": 1516
    },
    {
      "epoch": 8.295081967213115,
      "grad_norm": 13.609575271606445,
      "learning_rate": 8.524590163934427e-06,
      "loss": 0.9746,
      "step": 1518
    },
    {
      "epoch": 8.306010928961749,
      "grad_norm": 11.770612716674805,
      "learning_rate": 8.469945355191257e-06,
      "loss": 0.6532,
      "step": 1520
    },
    {
      "epoch": 8.316939890710383,
      "grad_norm": 10.312310218811035,
      "learning_rate": 8.415300546448087e-06,
      "loss": 0.9113,
      "step": 1522
    },
    {
      "epoch": 8.327868852459016,
      "grad_norm": 12.742410659790039,
      "learning_rate": 8.360655737704919e-06,
      "loss": 0.8756,
      "step": 1524
    },
    {
      "epoch": 8.33879781420765,
      "grad_norm": 11.507115364074707,
      "learning_rate": 8.306010928961749e-06,
      "loss": 0.7794,
      "step": 1526
    },
    {
      "epoch": 8.349726775956285,
      "grad_norm": 11.471701622009277,
      "learning_rate": 8.25136612021858e-06,
      "loss": 0.8664,
      "step": 1528
    },
    {
      "epoch": 8.360655737704919,
      "grad_norm": 10.5148286819458,
      "learning_rate": 8.196721311475409e-06,
      "loss": 0.8042,
      "step": 1530
    },
    {
      "epoch": 8.371584699453551,
      "grad_norm": 12.655525207519531,
      "learning_rate": 8.14207650273224e-06,
      "loss": 0.9807,
      "step": 1532
    },
    {
      "epoch": 8.382513661202186,
      "grad_norm": 13.506708145141602,
      "learning_rate": 8.08743169398907e-06,
      "loss": 0.9417,
      "step": 1534
    },
    {
      "epoch": 8.39344262295082,
      "grad_norm": 13.323861122131348,
      "learning_rate": 8.032786885245902e-06,
      "loss": 0.9678,
      "step": 1536
    },
    {
      "epoch": 8.404371584699454,
      "grad_norm": 15.296378135681152,
      "learning_rate": 7.978142076502732e-06,
      "loss": 0.8902,
      "step": 1538
    },
    {
      "epoch": 8.415300546448087,
      "grad_norm": 13.407785415649414,
      "learning_rate": 7.923497267759564e-06,
      "loss": 1.1031,
      "step": 1540
    },
    {
      "epoch": 8.426229508196721,
      "grad_norm": 13.654522895812988,
      "learning_rate": 7.868852459016394e-06,
      "loss": 1.0217,
      "step": 1542
    },
    {
      "epoch": 8.437158469945356,
      "grad_norm": 13.430536270141602,
      "learning_rate": 7.814207650273224e-06,
      "loss": 1.034,
      "step": 1544
    },
    {
      "epoch": 8.448087431693988,
      "grad_norm": 13.23009204864502,
      "learning_rate": 7.759562841530054e-06,
      "loss": 1.0604,
      "step": 1546
    },
    {
      "epoch": 8.459016393442623,
      "grad_norm": 13.291468620300293,
      "learning_rate": 7.704918032786886e-06,
      "loss": 0.8694,
      "step": 1548
    },
    {
      "epoch": 8.469945355191257,
      "grad_norm": 14.730644226074219,
      "learning_rate": 7.650273224043716e-06,
      "loss": 0.9698,
      "step": 1550
    },
    {
      "epoch": 8.480874316939891,
      "grad_norm": 11.78342342376709,
      "learning_rate": 7.595628415300547e-06,
      "loss": 1.0147,
      "step": 1552
    },
    {
      "epoch": 8.491803278688524,
      "grad_norm": 14.848114013671875,
      "learning_rate": 7.540983606557378e-06,
      "loss": 1.1852,
      "step": 1554
    },
    {
      "epoch": 8.502732240437158,
      "grad_norm": 13.258012771606445,
      "learning_rate": 7.486338797814208e-06,
      "loss": 0.9673,
      "step": 1556
    },
    {
      "epoch": 8.513661202185792,
      "grad_norm": 12.053437232971191,
      "learning_rate": 7.431693989071039e-06,
      "loss": 0.8329,
      "step": 1558
    },
    {
      "epoch": 8.524590163934427,
      "grad_norm": 11.751015663146973,
      "learning_rate": 7.3770491803278695e-06,
      "loss": 0.9315,
      "step": 1560
    },
    {
      "epoch": 8.53551912568306,
      "grad_norm": 13.98900032043457,
      "learning_rate": 7.3224043715846995e-06,
      "loss": 1.0276,
      "step": 1562
    },
    {
      "epoch": 8.546448087431694,
      "grad_norm": 13.174354553222656,
      "learning_rate": 7.26775956284153e-06,
      "loss": 1.0402,
      "step": 1564
    },
    {
      "epoch": 8.557377049180328,
      "grad_norm": 12.379037857055664,
      "learning_rate": 7.213114754098361e-06,
      "loss": 1.0263,
      "step": 1566
    },
    {
      "epoch": 8.568306010928962,
      "grad_norm": 13.953060150146484,
      "learning_rate": 7.158469945355191e-06,
      "loss": 1.1066,
      "step": 1568
    },
    {
      "epoch": 8.579234972677595,
      "grad_norm": 12.800646781921387,
      "learning_rate": 7.103825136612022e-06,
      "loss": 1.0213,
      "step": 1570
    },
    {
      "epoch": 8.59016393442623,
      "grad_norm": 13.33777141571045,
      "learning_rate": 7.049180327868852e-06,
      "loss": 1.1424,
      "step": 1572
    },
    {
      "epoch": 8.601092896174864,
      "grad_norm": 9.928971290588379,
      "learning_rate": 6.994535519125683e-06,
      "loss": 0.8583,
      "step": 1574
    },
    {
      "epoch": 8.612021857923498,
      "grad_norm": 13.136619567871094,
      "learning_rate": 6.939890710382514e-06,
      "loss": 0.857,
      "step": 1576
    },
    {
      "epoch": 8.62295081967213,
      "grad_norm": 13.619216918945312,
      "learning_rate": 6.885245901639345e-06,
      "loss": 0.9444,
      "step": 1578
    },
    {
      "epoch": 8.633879781420765,
      "grad_norm": 12.012370109558105,
      "learning_rate": 6.830601092896176e-06,
      "loss": 0.9128,
      "step": 1580
    },
    {
      "epoch": 8.6448087431694,
      "grad_norm": 15.056665420532227,
      "learning_rate": 6.775956284153005e-06,
      "loss": 0.8825,
      "step": 1582
    },
    {
      "epoch": 8.655737704918034,
      "grad_norm": 13.37099552154541,
      "learning_rate": 6.721311475409836e-06,
      "loss": 0.9036,
      "step": 1584
    },
    {
      "epoch": 8.666666666666666,
      "grad_norm": 14.652631759643555,
      "learning_rate": 6.666666666666667e-06,
      "loss": 1.0222,
      "step": 1586
    },
    {
      "epoch": 8.6775956284153,
      "grad_norm": 14.390247344970703,
      "learning_rate": 6.6120218579234975e-06,
      "loss": 1.1352,
      "step": 1588
    },
    {
      "epoch": 8.688524590163935,
      "grad_norm": 12.157851219177246,
      "learning_rate": 6.557377049180328e-06,
      "loss": 0.9495,
      "step": 1590
    },
    {
      "epoch": 8.699453551912569,
      "grad_norm": 14.349650382995605,
      "learning_rate": 6.502732240437159e-06,
      "loss": 0.7914,
      "step": 1592
    },
    {
      "epoch": 8.710382513661202,
      "grad_norm": 13.425882339477539,
      "learning_rate": 6.4480874316939885e-06,
      "loss": 0.8082,
      "step": 1594
    },
    {
      "epoch": 8.721311475409836,
      "grad_norm": 14.673821449279785,
      "learning_rate": 6.393442622950819e-06,
      "loss": 1.0964,
      "step": 1596
    },
    {
      "epoch": 8.73224043715847,
      "grad_norm": 14.556866645812988,
      "learning_rate": 6.33879781420765e-06,
      "loss": 0.9082,
      "step": 1598
    },
    {
      "epoch": 8.743169398907105,
      "grad_norm": 13.105338096618652,
      "learning_rate": 6.284153005464481e-06,
      "loss": 0.9248,
      "step": 1600
    },
    {
      "epoch": 8.754098360655737,
      "grad_norm": 13.086469650268555,
      "learning_rate": 6.229508196721312e-06,
      "loss": 0.7868,
      "step": 1602
    },
    {
      "epoch": 8.765027322404372,
      "grad_norm": 12.962210655212402,
      "learning_rate": 6.174863387978142e-06,
      "loss": 0.9959,
      "step": 1604
    },
    {
      "epoch": 8.775956284153006,
      "grad_norm": 12.038783073425293,
      "learning_rate": 6.120218579234973e-06,
      "loss": 0.7054,
      "step": 1606
    },
    {
      "epoch": 8.78688524590164,
      "grad_norm": 10.313270568847656,
      "learning_rate": 6.065573770491804e-06,
      "loss": 0.8046,
      "step": 1608
    },
    {
      "epoch": 8.797814207650273,
      "grad_norm": 13.121846199035645,
      "learning_rate": 6.010928961748634e-06,
      "loss": 0.7876,
      "step": 1610
    },
    {
      "epoch": 8.808743169398907,
      "grad_norm": 11.481009483337402,
      "learning_rate": 5.956284153005465e-06,
      "loss": 0.6791,
      "step": 1612
    },
    {
      "epoch": 8.819672131147541,
      "grad_norm": 12.665916442871094,
      "learning_rate": 5.9016393442622956e-06,
      "loss": 1.0106,
      "step": 1614
    },
    {
      "epoch": 8.830601092896174,
      "grad_norm": 13.853859901428223,
      "learning_rate": 5.846994535519126e-06,
      "loss": 1.1072,
      "step": 1616
    },
    {
      "epoch": 8.841530054644808,
      "grad_norm": 13.95446491241455,
      "learning_rate": 5.7923497267759565e-06,
      "loss": 0.9441,
      "step": 1618
    },
    {
      "epoch": 8.852459016393443,
      "grad_norm": 13.744958877563477,
      "learning_rate": 5.737704918032787e-06,
      "loss": 0.8256,
      "step": 1620
    },
    {
      "epoch": 8.863387978142077,
      "grad_norm": 12.895857810974121,
      "learning_rate": 5.683060109289618e-06,
      "loss": 0.6888,
      "step": 1622
    },
    {
      "epoch": 8.87431693989071,
      "grad_norm": 11.517133712768555,
      "learning_rate": 5.628415300546448e-06,
      "loss": 0.7795,
      "step": 1624
    },
    {
      "epoch": 8.885245901639344,
      "grad_norm": 14.558837890625,
      "learning_rate": 5.573770491803279e-06,
      "loss": 1.2372,
      "step": 1626
    },
    {
      "epoch": 8.896174863387978,
      "grad_norm": 14.036397933959961,
      "learning_rate": 5.51912568306011e-06,
      "loss": 0.9759,
      "step": 1628
    },
    {
      "epoch": 8.907103825136613,
      "grad_norm": 11.829805374145508,
      "learning_rate": 5.46448087431694e-06,
      "loss": 0.849,
      "step": 1630
    },
    {
      "epoch": 8.918032786885245,
      "grad_norm": 14.684367179870605,
      "learning_rate": 5.409836065573771e-06,
      "loss": 0.969,
      "step": 1632
    },
    {
      "epoch": 8.92896174863388,
      "grad_norm": 13.478360176086426,
      "learning_rate": 5.355191256830602e-06,
      "loss": 0.8773,
      "step": 1634
    },
    {
      "epoch": 8.939890710382514,
      "grad_norm": 14.095932006835938,
      "learning_rate": 5.300546448087432e-06,
      "loss": 1.0175,
      "step": 1636
    },
    {
      "epoch": 8.950819672131148,
      "grad_norm": 12.065247535705566,
      "learning_rate": 5.245901639344263e-06,
      "loss": 1.0319,
      "step": 1638
    },
    {
      "epoch": 8.96174863387978,
      "grad_norm": 10.772770881652832,
      "learning_rate": 5.191256830601094e-06,
      "loss": 0.7647,
      "step": 1640
    },
    {
      "epoch": 8.972677595628415,
      "grad_norm": 15.644990921020508,
      "learning_rate": 5.136612021857924e-06,
      "loss": 1.0689,
      "step": 1642
    },
    {
      "epoch": 8.98360655737705,
      "grad_norm": 12.22131061553955,
      "learning_rate": 5.0819672131147545e-06,
      "loss": 1.1265,
      "step": 1644
    },
    {
      "epoch": 8.994535519125684,
      "grad_norm": 11.562365531921387,
      "learning_rate": 5.027322404371585e-06,
      "loss": 0.9006,
      "step": 1646
    },
    {
      "epoch": 9.005464480874316,
      "grad_norm": 12.171196937561035,
      "learning_rate": 4.9726775956284154e-06,
      "loss": 0.9336,
      "step": 1648
    },
    {
      "epoch": 9.01639344262295,
      "grad_norm": 14.190109252929688,
      "learning_rate": 4.918032786885246e-06,
      "loss": 0.8493,
      "step": 1650
    },
    {
      "epoch": 9.027322404371585,
      "grad_norm": 15.281068801879883,
      "learning_rate": 4.863387978142077e-06,
      "loss": 0.8387,
      "step": 1652
    },
    {
      "epoch": 9.03825136612022,
      "grad_norm": 15.094353675842285,
      "learning_rate": 4.808743169398908e-06,
      "loss": 1.1034,
      "step": 1654
    },
    {
      "epoch": 9.049180327868852,
      "grad_norm": 13.636332511901855,
      "learning_rate": 4.754098360655738e-06,
      "loss": 0.9692,
      "step": 1656
    },
    {
      "epoch": 9.060109289617486,
      "grad_norm": 13.249144554138184,
      "learning_rate": 4.699453551912569e-06,
      "loss": 0.9071,
      "step": 1658
    },
    {
      "epoch": 9.07103825136612,
      "grad_norm": 11.651936531066895,
      "learning_rate": 4.6448087431694e-06,
      "loss": 0.7933,
      "step": 1660
    },
    {
      "epoch": 9.081967213114755,
      "grad_norm": 11.524508476257324,
      "learning_rate": 4.59016393442623e-06,
      "loss": 0.884,
      "step": 1662
    },
    {
      "epoch": 9.092896174863387,
      "grad_norm": 13.917519569396973,
      "learning_rate": 4.535519125683061e-06,
      "loss": 0.9713,
      "step": 1664
    },
    {
      "epoch": 9.103825136612022,
      "grad_norm": 14.6138277053833,
      "learning_rate": 4.480874316939891e-06,
      "loss": 0.8466,
      "step": 1666
    },
    {
      "epoch": 9.114754098360656,
      "grad_norm": 13.661123275756836,
      "learning_rate": 4.426229508196722e-06,
      "loss": 0.9132,
      "step": 1668
    },
    {
      "epoch": 9.12568306010929,
      "grad_norm": 12.652124404907227,
      "learning_rate": 4.371584699453552e-06,
      "loss": 0.8677,
      "step": 1670
    },
    {
      "epoch": 9.136612021857923,
      "grad_norm": 10.226336479187012,
      "learning_rate": 4.316939890710383e-06,
      "loss": 0.7997,
      "step": 1672
    },
    {
      "epoch": 9.147540983606557,
      "grad_norm": 15.071141242980957,
      "learning_rate": 4.2622950819672135e-06,
      "loss": 1.0785,
      "step": 1674
    },
    {
      "epoch": 9.158469945355192,
      "grad_norm": 8.7422513961792,
      "learning_rate": 4.2076502732240435e-06,
      "loss": 0.7191,
      "step": 1676
    },
    {
      "epoch": 9.169398907103826,
      "grad_norm": 10.918065071105957,
      "learning_rate": 4.153005464480874e-06,
      "loss": 0.8598,
      "step": 1678
    },
    {
      "epoch": 9.180327868852459,
      "grad_norm": 13.158026695251465,
      "learning_rate": 4.098360655737704e-06,
      "loss": 0.9281,
      "step": 1680
    },
    {
      "epoch": 9.191256830601093,
      "grad_norm": 14.344254493713379,
      "learning_rate": 4.043715846994535e-06,
      "loss": 0.8781,
      "step": 1682
    },
    {
      "epoch": 9.202185792349727,
      "grad_norm": 13.497621536254883,
      "learning_rate": 3.989071038251366e-06,
      "loss": 0.9549,
      "step": 1684
    },
    {
      "epoch": 9.21311475409836,
      "grad_norm": 12.863253593444824,
      "learning_rate": 3.934426229508197e-06,
      "loss": 0.8387,
      "step": 1686
    },
    {
      "epoch": 9.224043715846994,
      "grad_norm": 13.613567352294922,
      "learning_rate": 3.879781420765027e-06,
      "loss": 0.885,
      "step": 1688
    },
    {
      "epoch": 9.234972677595628,
      "grad_norm": 13.647848129272461,
      "learning_rate": 3.825136612021858e-06,
      "loss": 0.9054,
      "step": 1690
    },
    {
      "epoch": 9.245901639344263,
      "grad_norm": 14.6072998046875,
      "learning_rate": 3.770491803278689e-06,
      "loss": 0.9889,
      "step": 1692
    },
    {
      "epoch": 9.256830601092895,
      "grad_norm": 14.038522720336914,
      "learning_rate": 3.7158469945355193e-06,
      "loss": 0.9636,
      "step": 1694
    },
    {
      "epoch": 9.26775956284153,
      "grad_norm": 13.400979995727539,
      "learning_rate": 3.6612021857923497e-06,
      "loss": 0.9498,
      "step": 1696
    },
    {
      "epoch": 9.278688524590164,
      "grad_norm": 13.33560562133789,
      "learning_rate": 3.6065573770491806e-06,
      "loss": 0.9147,
      "step": 1698
    },
    {
      "epoch": 9.289617486338798,
      "grad_norm": 13.111295700073242,
      "learning_rate": 3.551912568306011e-06,
      "loss": 0.9254,
      "step": 1700
    },
    {
      "epoch": 9.300546448087431,
      "grad_norm": 11.39143180847168,
      "learning_rate": 3.4972677595628415e-06,
      "loss": 0.8493,
      "step": 1702
    },
    {
      "epoch": 9.311475409836065,
      "grad_norm": 13.567951202392578,
      "learning_rate": 3.4426229508196724e-06,
      "loss": 0.9483,
      "step": 1704
    },
    {
      "epoch": 9.3224043715847,
      "grad_norm": 12.829122543334961,
      "learning_rate": 3.3879781420765024e-06,
      "loss": 0.9408,
      "step": 1706
    },
    {
      "epoch": 9.333333333333334,
      "grad_norm": 11.253517150878906,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.8731,
      "step": 1708
    },
    {
      "epoch": 9.344262295081966,
      "grad_norm": 8.99229621887207,
      "learning_rate": 3.278688524590164e-06,
      "loss": 0.7922,
      "step": 1710
    },
    {
      "epoch": 9.3551912568306,
      "grad_norm": 12.553685188293457,
      "learning_rate": 3.2240437158469942e-06,
      "loss": 0.9164,
      "step": 1712
    },
    {
      "epoch": 9.366120218579235,
      "grad_norm": 12.007185935974121,
      "learning_rate": 3.169398907103825e-06,
      "loss": 0.7084,
      "step": 1714
    },
    {
      "epoch": 9.37704918032787,
      "grad_norm": 11.531445503234863,
      "learning_rate": 3.114754098360656e-06,
      "loss": 0.7324,
      "step": 1716
    },
    {
      "epoch": 9.387978142076502,
      "grad_norm": 11.051429748535156,
      "learning_rate": 3.0601092896174864e-06,
      "loss": 0.7505,
      "step": 1718
    },
    {
      "epoch": 9.398907103825136,
      "grad_norm": 12.822848320007324,
      "learning_rate": 3.005464480874317e-06,
      "loss": 1.0112,
      "step": 1720
    },
    {
      "epoch": 9.40983606557377,
      "grad_norm": 8.346431732177734,
      "learning_rate": 2.9508196721311478e-06,
      "loss": 0.8735,
      "step": 1722
    },
    {
      "epoch": 9.420765027322405,
      "grad_norm": 10.286441802978516,
      "learning_rate": 2.8961748633879782e-06,
      "loss": 0.8706,
      "step": 1724
    },
    {
      "epoch": 9.431693989071038,
      "grad_norm": 14.455183982849121,
      "learning_rate": 2.841530054644809e-06,
      "loss": 1.0462,
      "step": 1726
    },
    {
      "epoch": 9.442622950819672,
      "grad_norm": 13.772153854370117,
      "learning_rate": 2.7868852459016396e-06,
      "loss": 0.9839,
      "step": 1728
    },
    {
      "epoch": 9.453551912568306,
      "grad_norm": 10.571019172668457,
      "learning_rate": 2.73224043715847e-06,
      "loss": 0.8314,
      "step": 1730
    },
    {
      "epoch": 9.46448087431694,
      "grad_norm": 14.169459342956543,
      "learning_rate": 2.677595628415301e-06,
      "loss": 0.9296,
      "step": 1732
    },
    {
      "epoch": 9.475409836065573,
      "grad_norm": 11.107686042785645,
      "learning_rate": 2.6229508196721314e-06,
      "loss": 0.6274,
      "step": 1734
    },
    {
      "epoch": 9.486338797814208,
      "grad_norm": 10.85430908203125,
      "learning_rate": 2.568306010928962e-06,
      "loss": 0.7681,
      "step": 1736
    },
    {
      "epoch": 9.497267759562842,
      "grad_norm": 11.47761344909668,
      "learning_rate": 2.5136612021857927e-06,
      "loss": 0.8967,
      "step": 1738
    },
    {
      "epoch": 9.508196721311476,
      "grad_norm": 14.074817657470703,
      "learning_rate": 2.459016393442623e-06,
      "loss": 0.8805,
      "step": 1740
    },
    {
      "epoch": 9.519125683060109,
      "grad_norm": 14.436525344848633,
      "learning_rate": 2.404371584699454e-06,
      "loss": 0.9788,
      "step": 1742
    },
    {
      "epoch": 9.530054644808743,
      "grad_norm": 12.640711784362793,
      "learning_rate": 2.3497267759562845e-06,
      "loss": 0.982,
      "step": 1744
    },
    {
      "epoch": 9.540983606557377,
      "grad_norm": 13.77420711517334,
      "learning_rate": 2.295081967213115e-06,
      "loss": 0.9439,
      "step": 1746
    },
    {
      "epoch": 9.551912568306012,
      "grad_norm": 11.747513771057129,
      "learning_rate": 2.2404371584699454e-06,
      "loss": 0.7555,
      "step": 1748
    },
    {
      "epoch": 9.562841530054644,
      "grad_norm": 11.140860557556152,
      "learning_rate": 2.185792349726776e-06,
      "loss": 0.8423,
      "step": 1750
    },
    {
      "epoch": 9.573770491803279,
      "grad_norm": 13.503416061401367,
      "learning_rate": 2.1311475409836067e-06,
      "loss": 0.8991,
      "step": 1752
    },
    {
      "epoch": 9.584699453551913,
      "grad_norm": 12.11516284942627,
      "learning_rate": 2.076502732240437e-06,
      "loss": 0.9156,
      "step": 1754
    },
    {
      "epoch": 9.595628415300546,
      "grad_norm": 10.988493919372559,
      "learning_rate": 2.0218579234972676e-06,
      "loss": 0.8122,
      "step": 1756
    },
    {
      "epoch": 9.60655737704918,
      "grad_norm": 12.779623031616211,
      "learning_rate": 1.9672131147540985e-06,
      "loss": 1.0237,
      "step": 1758
    },
    {
      "epoch": 9.617486338797814,
      "grad_norm": 14.683313369750977,
      "learning_rate": 1.912568306010929e-06,
      "loss": 0.9339,
      "step": 1760
    },
    {
      "epoch": 9.628415300546449,
      "grad_norm": 12.597020149230957,
      "learning_rate": 1.8579234972677596e-06,
      "loss": 1.0215,
      "step": 1762
    },
    {
      "epoch": 9.639344262295083,
      "grad_norm": 13.320239067077637,
      "learning_rate": 1.8032786885245903e-06,
      "loss": 0.9132,
      "step": 1764
    },
    {
      "epoch": 9.650273224043715,
      "grad_norm": 9.92306137084961,
      "learning_rate": 1.7486338797814208e-06,
      "loss": 0.7923,
      "step": 1766
    },
    {
      "epoch": 9.66120218579235,
      "grad_norm": 11.885586738586426,
      "learning_rate": 1.6939890710382512e-06,
      "loss": 0.8279,
      "step": 1768
    },
    {
      "epoch": 9.672131147540984,
      "grad_norm": 13.228982925415039,
      "learning_rate": 1.639344262295082e-06,
      "loss": 0.9965,
      "step": 1770
    },
    {
      "epoch": 9.683060109289617,
      "grad_norm": 13.7826509475708,
      "learning_rate": 1.5846994535519126e-06,
      "loss": 0.9533,
      "step": 1772
    },
    {
      "epoch": 9.693989071038251,
      "grad_norm": 9.830938339233398,
      "learning_rate": 1.5300546448087432e-06,
      "loss": 0.6695,
      "step": 1774
    },
    {
      "epoch": 9.704918032786885,
      "grad_norm": 12.214431762695312,
      "learning_rate": 1.4754098360655739e-06,
      "loss": 0.9285,
      "step": 1776
    },
    {
      "epoch": 9.71584699453552,
      "grad_norm": 12.28459644317627,
      "learning_rate": 1.4207650273224046e-06,
      "loss": 0.7447,
      "step": 1778
    },
    {
      "epoch": 9.726775956284152,
      "grad_norm": 9.484434127807617,
      "learning_rate": 1.366120218579235e-06,
      "loss": 0.722,
      "step": 1780
    },
    {
      "epoch": 9.737704918032787,
      "grad_norm": 10.206586837768555,
      "learning_rate": 1.3114754098360657e-06,
      "loss": 0.8172,
      "step": 1782
    },
    {
      "epoch": 9.748633879781421,
      "grad_norm": 13.31507396697998,
      "learning_rate": 1.2568306010928963e-06,
      "loss": 1.0649,
      "step": 1784
    },
    {
      "epoch": 9.759562841530055,
      "grad_norm": 12.210338592529297,
      "learning_rate": 1.202185792349727e-06,
      "loss": 0.761,
      "step": 1786
    },
    {
      "epoch": 9.770491803278688,
      "grad_norm": 9.354406356811523,
      "learning_rate": 1.1475409836065575e-06,
      "loss": 0.7336,
      "step": 1788
    },
    {
      "epoch": 9.781420765027322,
      "grad_norm": 12.371045112609863,
      "learning_rate": 1.092896174863388e-06,
      "loss": 0.9495,
      "step": 1790
    },
    {
      "epoch": 9.792349726775956,
      "grad_norm": 10.41491413116455,
      "learning_rate": 1.0382513661202186e-06,
      "loss": 0.909,
      "step": 1792
    },
    {
      "epoch": 9.80327868852459,
      "grad_norm": 9.37287712097168,
      "learning_rate": 9.836065573770493e-07,
      "loss": 0.7652,
      "step": 1794
    },
    {
      "epoch": 9.814207650273223,
      "grad_norm": 11.465234756469727,
      "learning_rate": 9.289617486338798e-07,
      "loss": 0.6978,
      "step": 1796
    },
    {
      "epoch": 9.825136612021858,
      "grad_norm": 11.95094108581543,
      "learning_rate": 8.743169398907104e-07,
      "loss": 0.6314,
      "step": 1798
    },
    {
      "epoch": 9.836065573770492,
      "grad_norm": 10.634905815124512,
      "learning_rate": 8.19672131147541e-07,
      "loss": 0.8993,
      "step": 1800
    },
    {
      "epoch": 9.846994535519126,
      "grad_norm": 10.641539573669434,
      "learning_rate": 7.650273224043716e-07,
      "loss": 0.745,
      "step": 1802
    },
    {
      "epoch": 9.857923497267759,
      "grad_norm": 12.725111961364746,
      "learning_rate": 7.103825136612023e-07,
      "loss": 1.0285,
      "step": 1804
    },
    {
      "epoch": 9.868852459016393,
      "grad_norm": 14.39930248260498,
      "learning_rate": 6.557377049180328e-07,
      "loss": 1.057,
      "step": 1806
    },
    {
      "epoch": 9.879781420765028,
      "grad_norm": 14.67743968963623,
      "learning_rate": 6.010928961748635e-07,
      "loss": 0.9493,
      "step": 1808
    },
    {
      "epoch": 9.890710382513662,
      "grad_norm": 10.51276683807373,
      "learning_rate": 5.46448087431694e-07,
      "loss": 0.6782,
      "step": 1810
    },
    {
      "epoch": 9.901639344262295,
      "grad_norm": 13.085428237915039,
      "learning_rate": 4.918032786885246e-07,
      "loss": 0.9093,
      "step": 1812
    },
    {
      "epoch": 9.912568306010929,
      "grad_norm": 11.479172706604004,
      "learning_rate": 4.371584699453552e-07,
      "loss": 0.602,
      "step": 1814
    },
    {
      "epoch": 9.923497267759563,
      "grad_norm": 13.95228099822998,
      "learning_rate": 3.825136612021858e-07,
      "loss": 0.9353,
      "step": 1816
    },
    {
      "epoch": 9.934426229508198,
      "grad_norm": 10.424699783325195,
      "learning_rate": 3.278688524590164e-07,
      "loss": 0.7898,
      "step": 1818
    },
    {
      "epoch": 9.94535519125683,
      "grad_norm": 12.666090965270996,
      "learning_rate": 2.73224043715847e-07,
      "loss": 0.8102,
      "step": 1820
    },
    {
      "epoch": 9.956284153005464,
      "grad_norm": 10.203243255615234,
      "learning_rate": 2.185792349726776e-07,
      "loss": 0.9422,
      "step": 1822
    },
    {
      "epoch": 9.967213114754099,
      "grad_norm": 14.61596393585205,
      "learning_rate": 1.639344262295082e-07,
      "loss": 0.8576,
      "step": 1824
    },
    {
      "epoch": 9.978142076502731,
      "grad_norm": 13.65131950378418,
      "learning_rate": 1.092896174863388e-07,
      "loss": 1.2239,
      "step": 1826
    },
    {
      "epoch": 9.989071038251366,
      "grad_norm": 12.608604431152344,
      "learning_rate": 5.46448087431694e-08,
      "loss": 1.0107,
      "step": 1828
    },
    {
      "epoch": 10.0,
      "grad_norm": 10.480304718017578,
      "learning_rate": 0.0,
      "loss": 0.8465,
      "step": 1830
    }
  ],
  "logging_steps": 2,
  "max_steps": 1830,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 119541104640000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
